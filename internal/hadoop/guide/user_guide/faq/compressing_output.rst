******************
Compressing Output
******************

.. contents:: Table of Contents
  :local:
  :depth: 4

-----------

:guilabel:`Compression Schemes`

To conserve space on HDFS, compress your output files. Available compression schemes include:

* Hadoop 0.18.* - Gzip and Lzo
* Hadoop 0.20 - Gzip, Lzo, and Bzip2

:guilabel:`Job Output v. Intermediate Map Output`

* Job output refers to the reduce output (``part-*``) files of a Map/Reduce Job or a Map output (``part-*``) files of a Map only job.
  In both cases job output files are written to the HDFS.
* Intermediate Map output in a Map/Reduce job refers to the local output files generated by the Map phase and consumed internally by the Reduce phase.

:guilabel:`General Guidelines`

* Typically you need not worry about the compression of intermediate map output data.
  It is by default turned ON and uses LZO compression scheme. Reducers automatically decompress and consume this compressed output from the map phase.
  The following default parameter values are set on the Yahoo clusters to enable intermediate map output compression:

   * ``mapred.compress.map.output = true``
   * ``mapred.map.output.compression.codec = org.apache.hadoop.io.compress.LzoCodec``

* Typically you should use either Gzip or Bzip2 compression scheme to compress your job output files.
  From the grid or Map/Reduce processing standpoint, the following is an important difference:

   * Gzip compressed input file can not be split across multiple maps. One map instance will be created to consume single gzipped input file and thus may restrict the granularity of the map task processing.
   * Bizp2 compressed input file can be split across multiple maps and thus recommended for processing the data on the grid.

* If you set the option e.g. job conf parameter ``mapred.output.compress=true`` to compress your job output, but don't set the compression scheme e.g. ``mapred.output.compression.codec``, then default compression scheme used is Gzip.

* To compress your existing uncompressed **TEXT** data on the Grid use the data compression tool (note that this tool is not useful for compressing the sequence files).

The following sections show how to compress job output files and how to subsequently consume them through another Map/Reduce (or Map only) job.


How to Compress Job Output and Intermediate Map Output
======================================================


:guilabel:`Map/Reduce`

.. todo:: fix link Data Compression

* For Java Map/Reduce jobs, see the Map/Reduce tutorial for an example of how to set compression: `Data Compression <https://archives.ouroath.com/twiki/twiki.corp.yahoo.com:8080/?url=http%3A%2F%2Fhadoop.apache.org%2Fcore%2Fdocs%2Fcurrent%2Fmapred_tutorial.html%23Data%2BCompression&SIG=1239ulk9a/>`_

* Note: For Intermediate (Map) outputs you may also need to set the following Job (Map/Reduce) output parameters:

  .. code-block:: bash

    FileOutputFormat.setCompressOutput(compressJob, true);
    FileOutputFormat.setOutputCompressorClass(compressJob,org.apache.hadoop.io.compress.GzipCodec.class);
    compressJob.setNumReduceTasks(0);


:guilabel:`Streaming`


For Streaming jobs, compression parameters can be specified through ``â€“jobconf`` (``-D`` option in Hadoop 0.20):

* To enable job output compression and set the appropriate compression scheme (for both text and sequence files):

   * Set ``mapred.output.compress=true``
   * Set ``mapred.output.compression.codec`` (supported codecs : ``org.apache.hadoop.io.compress.GzipCodec``, ``org.apache.hadoop.io.compress.BZip2Codec``)

* For sequence files, the following are additional optional parameters to set for BLOCK type compression, where the default is RECORD type (see SequenceFile for more details):

   * Set ``mapred.output.compression.type=BLOCK``
   * Set ``io.seqfile.compress.blocksize`` (The minimum block size for compression in block compressed ``SequenceFiles``, default 1MB)
   * Set ``io.seqfile.lazydecompress`` (Should values of block-compressed ``SequenceFiles`` be decompressed only when necessary, default true)


.. seealso:: See the Streaming doc for an example of how to set the compression: `How do I generate output files with gzip format? <http://twiki.corp.yahoo.com:8080/?url=http%3A%2F%2Fhadoop.apache.org%2Fcore%2Fdocs%2Fcurrent%2Fstreaming.html%23How%2Bdo%2BI%2Bgenerate%2Boutput%2Bfiles%2Bwith%2Bgzip%2Bformat%3F&SIG=11t1p1qii/>`_ and :ref:`guide_faq_compressing_output_of_streaming_job`

:guilabel:`Pig`

Pig currently supports the bzip2 for output compression (gzip output compression does not work and a JIRA exists to fix it). For Pig jobs, set the name of your output file with extension .bz2, e.g ``xyz.bz2``. See details in `twiki pig page <https://archives.ouroath.com/twiki/twiki.corp.yahoo.com/view/Grid/CookBook#Can_I_save_the_output_files_from>`_.


How to Read the Compressed Data into Map/Reduce Job
===================================================

* For Pig and Map/Reduce programs you don't need any special parameter settings to read the compressed files in Gzip/Bzip2 format (if you're using "``PigStorage``" to load your data.
  With other loaders such as ``XMLLoader`` for example, this automatic decompression will not happen).
  Both these schemes are supported in Hadoop 0.20, where mappers automatically decompress (based on input file extension .gz or .bz2) and consume the compressed data stored in the input directory.

   * However, remember: a gzip file will not be split across the maps and hence one gzip file will be processed per map; a bzip2 file will be split across the map tasks.

   .. note:: JIRA (HADOOP-4010 ) fixes this problem with bzip2 compressed text input files, which are currently not being split, i.e. one input file per map. Fix is committed for Hadoop 0.21.0).

* For Hadoop 0.18, Bzip2 codec is not supported by the framework. In this case, to read the bzip2 input make use of the ``bzip2.jar`` explicitly provided under solutions home.
   
   * See :ref:`guide_faq_compressing_output_how_use_bzip2_compression_on_grid` for details on how to read bzip2 files in Hadoop 0.18.
     
* Note: ``Bzip2`` codec input format does not support the bzip2 compressed file that is created by concatenating multiple ``.bz2`` files.
  It currently does NOT through any error for such concatenated bzip compressed files but silently skips the records. See following JIRAs opened to fix the problem HADOOP-5601, HADOOP-5602.     

Data Compression Tool
=====================

A data compression tool, ``compressFiles.pl``, is ( or will soon be) available under solutions home on all grid gateway machines: ``$SOLUTIONS_HOME/bin/compressFiles.pl``.
The tool is a streaming M/R job that compresses TEXT files under a specified HDFS input directory (this tool is not useful for compressing the sequence files). The tool supports bzip2 and gzip compression schemes along with various options to select the files to be compressed. For more details and a list of the supported options, use the help option: ``compressFile.pl --help``


..  _guide_faq_compressing_output_how_use_bzip2_compression_on_grid:

How do I use bzip2 compression scheme on the Grid ?
===================================================

``BZIP2`` codec is supported in hadoop-0.20.0 (set io.compression.codecs=org.apache.hadoop.io.compress.BZip2Codec) but until then you can use the bzip2.jar from $SOLUTIONS_HOME/jars. It provides bzip2 input format to split and read the input bzip files.

E.g. Streaming command:

  .. code-block:: bash

    #Before hadoop 0.19, need to set this class path on client side
    setenv HADOOP_CLASSPATH $SOLUTIONS_HOME/jars/bzip2.jar
    hadoop jar -libjars $SOLUTIONS_HOME/jars/bzip2.jar $HADOOP_HOME/hadoop-streaming.jar \
               -input /user/gogate/temp/testfile.txt.bz2 \
               -inputformat org.apache.hadoop.mapred.Bzip2TextInputFormat \
               -output /user/gogate/temp_out -mapper '/bin/cat' -reducer NONE


``$SOLUTIONS_HOME/jars/bzip2.jar`` does not provide ``bzip2`` output format. Although grid solutions team provides compressFiles tool to explicitly compress the job output text files into gzip or bzip format.

