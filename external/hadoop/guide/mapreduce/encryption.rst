..  _mapreduce_encryption:

**********
Encryption
**********

.. contents:: Table of Contents
  :local:
  :depth: 3

-----------


.. warning::
   * Using ``DefaultSpillKeyProvider`` is **ONLY** meant for testing purposes.
   * Enabling encryption for intermediate data spills automatically restricts
     the number of job attempts to 1.
   * Encrypting intermediate data (shuffle/spill) will incur a
     significant performance impact. |br|
     Users should profile this and potentially reserve 1 or more cores for
     encrypted shuffle.

..  _mapreduce_encryption_definitions:

.. topic:: Terminology
   :class: definitionbox

   .. glossary::

      Shuffling
        As described earlier, :term:`Shuffle` is the process of transfering data
        from the mappers to the reducers. :term:`Reducer` gets 1 or more keys
        and associated values on the basis of reducers. Intermediated key-value
        generated by mapper is sorted automatically by key.

      Sorting
        the :term:`Sort` happens simultaneously with the Shuffling. It makes the
        :term:`Reducer` job a lot easier. The latter takes a list of key-value
        pairs, but it has to call the ``reduce()`` method which takes a
        key-list(value) input, so it has to group values by key.

      Partitioning
        It determines in which reducer a (key, value) pair, output of the map
        phase, will be sent. The default Partitioner uses a hashing on the
        keys to distribute them to the reduce tasks.

      Spilling
        The :term:`Mapper` tries to fit all the output into the circular memory
        buffer (RAM). The size of the buffer is determined by
        ``mapreduce.task.io.sort.mb``. 
        Occasionally, a mapper's output exceeds a certain threshold of the amount
        of memory which was allocated for the MapReduce task.
        This triggers `Spilling`, which copies the :term:`Mapper` 's data from
        the memory buffer to disc.
        Spilling happens at least once when the mapper finished because the
        output of the mapper should be sorted and saved to the disk for
        :term:`Reducer` processes to read it. A `checkpoint` is required from
        which the reducers jobs can be restarted. The Checkpoint use those
        spilled records in case of a reduce task failure.

.. _mapreduce_shuffle_encryption:

Encrypted Shuffle
=================

The Encrypted Shuffle capability allows encryption of the MapReduce shuffle
using HTTPS and with optional client authentication (also known as
bi-directional HTTPS, or HTTPS with client certificates). It comprises:

* A Hadoop configuration setting for toggling the shuffle between HTTP and HTTPS.
* A Hadoop configuration settings for specifying the keystore and truststore
  properties (location, type, passwords) used by the shuffle service and the
  reducers tasks fetching shuffle data.
* A way to re-load truststores across the cluster (when a node is added or
  removed).


To enable encrypted shuffle, set ``mapreduce.shuffle.ssl.enabled`` to true
in `mapred-site.xml` of all nodes in the cluster. The default is false.


To configure encrypted shuffle, set the following properties in `core-site.xml`
of all nodes in the cluster:

.. table:: `core-site.xml Properties for shuffle encryption. Prefix 'hadoop.ssl'`
  :widths: auto
  :name: table-mr-shuffle-configs

  +-----------------------------+-------------------------------+-----------------------------------------------------------------------------------------+
  | Property                    | Default                       | Description                                                                             |
  +=============================+===============================+=========================================================================================+
  | ``require.client.cert``     | false                         | Whether client certificates are required                                                |
  +-----------------------------+-------------------------------+-----------------------------------------------------------------------------------------+
  | ``hostname.verifier``       | DEFAULT                       | The hostname verifier to provide for `HttpsURLConnections`. |br|                        |
  |                             |                               | Valid values are: |br|                                                                  |
  |                             |                               | `DEFAULT`, `STRICT`, `STRICT_IE6`, `DEFAULT_AND_LOCALHOST` and `ALLOW_ALL`              |
  +-----------------------------+-------------------------------+-----------------------------------------------------------------------------------------+
  | ``keystores.factory.class`` | ``FileBasedKeyStoresFactory`` | The KeyStoresFactory implementation to use                                              |
  +-----------------------------+-------------------------------+-----------------------------------------------------------------------------------------+
  | ``server.conf``             | `ssl-server.xml`              | Resource file from which ssl server keystore information will be extracted. |br|        |
  |                             |                               | This file is looked up in the classpath, it should be in Hadoop `conf`    directory     |
  +-----------------------------+-------------------------------+-----------------------------------------------------------------------------------------+
  | ``client.conf``             | `ssl-client.xml`              | Resource file from which ssl server keystore information will be extracted. |br|        |
  |                             |                               | This file is looked up in the classpath, it should be in Hadoop `conf/`   directory     |
  +-----------------------------+-------------------------------+-----------------------------------------------------------------------------------------+
  | ``enabled.protocols``       | TLSv1.2                       | The supported SSL protocols. The parameter will only be used from `DatanodeHttpServer`. |
  +-----------------------------+-------------------------------+-----------------------------------------------------------------------------------------+

.. note::
    * All these properties should be marked as final in the cluster configuration
      files including ``mapreduce.shuffle.ssl.enabled``.
    * The Linux container executor should be set to prevent job tasks from
      reading the server keystore information and gaining access to the shuffle
      server certificates.
    * Currently requiring client certificates should be set to `false`.
    * Refer to
      :ref:`Client Certificates section <mapreduce_shuffle_client_certificates>`
      for details



.. _mapreduce_shuffle_keystore_settings:

Keystore and Truststore Settings
--------------------------------

Currently ``FileBasedKeyStoresFactory`` is the only ``KeyStoresFactory``
implementation. The ``FileBasedKeyStoresFactory`` implementation uses the
following properties (:numref:`table-mr-shuffle-ssl-server`)
in the `ssl-server.xml` and `ssl-client.xml` files,
to configure the keystores and truststores.


The mapred user should own the `ssl-server.xml` file and have exclusive read
access to it.


.. table:: `ssl-server.xml. Prefix 'ssl.server'`
  :widths: auto
  :name: table-mr-shuffle-ssl-server

  +--------------------------------+---------+------------------------------------------------------------------------------------------------------+
  | Property                       | Default | Description                                                                                          |
  +================================+=========+======================================================================================================+
  | ``keystore.type``              | jks     | Keystore file type                                                                                   |
  +--------------------------------+---------+------------------------------------------------------------------------------------------------------+
  | ``keystore.location``          | NONE    | Keystore file location. The mapred user should own this file and have exclusive read access to it.   |
  +--------------------------------+---------+------------------------------------------------------------------------------------------------------+
  | ``keystore.password``          | NONE    | Keystore file password                                                                               |
  +--------------------------------+---------+------------------------------------------------------------------------------------------------------+
  | ``truststore.type``            | jks     | Truststore file type                                                                                 |
  +--------------------------------+---------+------------------------------------------------------------------------------------------------------+
  | ``truststore.location``        | NONE    | Truststore file location. The mapred user should own this file and have exclusive read access to it. |
  +--------------------------------+---------+------------------------------------------------------------------------------------------------------+
  | ``truststore.password``        | NONE    | Truststore file password                                                                             |
  +--------------------------------+---------+------------------------------------------------------------------------------------------------------+
  | ``truststore.reload.interval`` | 10000   | Truststore reload interval, in milliseconds                                                          |
  +--------------------------------+---------+------------------------------------------------------------------------------------------------------+  


The mapred user should own the `ssl-client.xml` file and it should have default
permissions. The values in that file are the same as descrived in
:numref:`table-mr-shuffle-ssl-server`.

.. _mapreduce_shuffle_client_certificates:

Client Certificates
-------------------

Using Client Certificates does not fully ensure that the client is a reducer
task for the job. Currently, Client Certificates (their private key) keystore
files must be readable by all users submitting jobs to the cluster. |br|
This means that a rogue job could read those keystore files and use the client
certificates in them to establish a secure connection with a Shuffle server.
However, unless the rogue job has a proper `JobToken`, it won’t be able to
retrieve shuffle data from the Shuffle server. A job, using its own `JobToken`,
can only retrieve shuffle data that belongs to itself.

By default the truststores will reload their configuration every 10 seconds.
If a new truststore file is copied over the old one, it will be re-read,
and its certificates will replace the old ones. |br|
This mechanism is useful for adding or removing nodes from the cluster,
or for adding or removing trusted clients. |br|
In these cases, the client or NodeManager certificate is added to
(or removed from) all the truststore files in the system, and the new
configuration will be picked up without you having to restart the `NodeManager`
daemons.


.. _mapreduce_spill_encryption:

Encrypted Spill
===============

A :term:`Spilling` is when a mapper's output exceeds the amount of memory which
was allocated for the MapReduce task. **Spilling** happens when there is not enough
memory to fit all the mapper output.

A Spilling thread writes the data to the file on the local drive of the server
where the mapper function is called. The directory to write is determined based
on the ``mapreduce.job.local.dir`` setting, which contains a list of the
directories to be used by the MR jobs on the cluster for temporary data.
One directory out of this list is chosen in a round robin fashion. 

MapReduce v2 allows to encrypt intermediate files generated during encrypted
shuffle and in case of data spills during the map and reduce stages.

.. _mapreduce_spill_encryption_expectations:

.. admonition:: What to expect with Encrypted Spilling...
   :class: readingbox

   * As mentioned, spilled files are writen to server's disc (not HDFS).
     This implies that the :term:`Mapper` outputs could be read easily.
     Encryption keeps the spilled temporary files secure.
   * Slow down in the throughput of the tasks due to the overhead of
     encrypting/decrypting the intermediate files.
   * :term:`Mapper` function could block if the mapper processing rate is greater
     that spilling rate, so that the memory buffer got 100% full
     (i.e., Spill thread takes long time encrypting data). In this case,
     :term:`Mapper` waits for the spill thread to free up some space.


This can be enabled by setting the following properties in `mapred-site.xml`.

.. table:: `Intermediate Data Spill configuration in MR. Prefixed 'mapreduce.job'`
  :widths: auto
  :name: table-mr-spill-configs

  +---------------------------------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------+
  | Configuration                                           | Type    | Description                                                                                                                    | Default                   |
  +=========================================================+=========+================================================================================================================================+===========================+
  | ``encrypted-intermediate`` |br| ``-data``               | boolean | Enable or disable encrypt intermediate mapreduce spill files.                                                                  | False                     |
  +---------------------------------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------+
  | ``spill-encryption`` |br| ``-keyprovider.class``        | string  | The class used as a spill key provider. This defines the encryption algorithm and key provider to get the client certificates. | `DefaultSpillKeyProvider` |
  +---------------------------------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------+
  | ``encrypted-intermediate`` |br| ``-data-key-size-bits`` | int     | The key length used to encrypt data spilled to disk.                                                                           | 128                       |
  +---------------------------------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------+
  | ``encrypted-intermediate`` |br| ``-data.buffer.kb``     | int     | The buffer size in kb for stream written to disk after encryption                                                              | 128                       |
  +---------------------------------------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------+---------------------------+

Available implementation for ``spill-encryption-keyprovider.class``:

* ``org.apache.hadoop.mapreduce.NullSpillKeyProvider``: a null encryption provider 
* ``org.apache.hadoop.mapreduce.DefaultSpillKeyProvider``: an implementation that uses
  ``HmacSHA1`` algo for encryption with a key length defined as
  ``encrypted-intermediate-data-key-size-bits``.
* ``org.apache.hadoop.mapreduce.KMSSpillKeyProvider``: KMS is used to provide
  intermediate encryption. In that case, the configuration
  ``mapreduce.job.kms-encryption-key-name`` provides the kms-key.


How to Use Encrypted Spill on a Job
-----------------------------------

The following two examples show command line to submit `TeraSort` job on AR.

Generate 6GB input folder:

  .. code-block:: bash

    ## login to the AR gateway
    ssh -A axonite-gw.red.ygrid.yahoo.com
    
    ## kinit as the user
    pkinit-user

    ## generate 6 GB of data
    hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
               teragen  \
               600000000 /tmp/terasort-input \
               -Dmapreduce.job.maps=16


Default Spill Encryption
^^^^^^^^^^^^^^^^^^^^^^^^

To run the job with default encryption key provider ``DefaultSpillKeyProvider``,
set ``encrypted-intermediate-data`` to true.

  .. code-block:: bash

    hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
                terasort \
                -Dmapreduce.job.maps=16 \
                -Dmapreduce.job.reduces=16 \
                -Dmapreduce.map.speculative=true \
                -Dmapreduce.reduce.speculative=true \
                -Dmapreduce.map.sort.spill.percent=0.8 \
                -Dmapreduce.reduce.shuffle.merge.percent=0.96 \
                -Dmapreduce.reduce.shuffle.input.buffer.percent=0.7 \
                -Dmapreduce.reduce.input.buffer.percent=0.96 \
                -Dmapreduce.job.reduce.slowstart.completedmaps=1.0 \
                -Dmapreduce.job.running.map.limit=32 \
                -Dmapreduce.job.encrypted-intermediate-data=true \
                -Dmapreduce.terasort.output.replication=3 \
                /tmp/terasort-input /tmp/terasort-output

  .. note::
    By default, the output of the
    :hadoop_rel_doc:`terasort reduce <api/org/apache/hadoop/examples/terasort/package-summary.html>`
    has replication set to 1, instead of the default 3. 
    Leaving replication set to 1 is not recommended. Therefore, make sure that
    ``-Dmapreduce.terasort.output.replication=3`` is passed to the job
    configurations.


KMS Spill Encryption
^^^^^^^^^^^^^^^^^^^^^

To run the job with KMS key provider, we need to set the following parameters:

* ``encrypted-intermediate-data``: true
* ``spill-encryption-keyprovider.class``: ``KMSSpillKeyProvider``
* ``kms-encryption-key-name``: `grid_us.EZ.spill_key`.

The `grid_us.EZ.spill_key <https://ui.ckms.ouroath.com/prod/view-keygroup/grid_us.EZ/view-key/grid_us.EZ.spill_key>`_
is created by gridops to serve as the key used to encrypt/decrypt spilled data
to disk.

  .. code-block:: bash

    hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
                terasort \
                -Dmapreduce.job.maps=16 \
                -Dmapreduce.job.reduces=16 \
                -Dmapreduce.map.speculative=true \
                -Dmapreduce.reduce.speculative=true \
                -Dmapreduce.map.sort.spill.percent=0.8 \
                -Dmapreduce.reduce.shuffle.merge.percent=0.96 \
                -Dmapreduce.reduce.shuffle.input.buffer.percent=0.7 \
                -Dmapreduce.reduce.input.buffer.percent=0.96 \
                -Dmapreduce.job.reduce.slowstart.completedmaps=1.0 \
                -Dmapreduce.job.running.map.limit=32 \
                -Dmapreduce.job.encrypted-intermediate-data=true \
                -Dmapreduce.job.kms-encryption-key-name=grid_us.EZ.spill_key \
                -Dmapreduce.terasort.output.replication=3 \
                /tmp/terasort-input /tmp/terasort-output

.. _mapreduce_encryption_evaluation:

Performance Evaluation
======================

Benchmarking Encryption on the grid
-----------------------------------

A `performance evaluation dated May 22nd 2019 <https://docs.google.com/spreadsheets/d/1dFdW3KrZD55rZo69oPaaZqcr1sr74SAsiNu5tSojCxk/edit#gid=2038478652>`_ of ``TeraSort``


Characteristics:
  * 11 GB and 20 GB shuffle 
  * To get the worst case scenario, the sort buffer size and factors were lowered

Results (the ones measured with smaller buffers):
  * 0.46%-9.3% (AVG: 2.45%) slow down post encryption feature;
  * Average Map times are in the 1.05-7% (AVG: 3.023%)
  * Average Reduce time change is around 0.85% -13% (AVG: 4.72%);
  * Total deterioration in shuffle and merge phase is ~ 4% in the worst case;
  * CPU times increases by 2.97%-10% (AVG: 4.43%)
  * CPU time degrades in the range of 0.7-2.26% (AVG: 1.55%).


.. _mapreduce_encryption_evaluation_optimization:

How to optimize shuffling/sorting phase in MR job
-------------------------------------------------

To achieve Shuffle and Sort efficiency:
  Combiner
     Using combiner will reduce the amount of data transferred to each of the
     reducers, since combiner merges the output on the mapper side.

  Number of reducers
     Choose optimal number of reducers. For further readings on how to pick the
     number of reducers, see:

     * `Hadoop wiki: HowManyMapsAndReduces <https://cwiki.apache.org/confluence/display/HADOOP2/HowManyMapsAndReduces>`_
     * `github.paulhoule/infovore: Choosing the number of reducers <https://github.com/paulhoule/infovore/wiki/Choosing-the-number-of-reducers>`_.

  Tuning ``mapreduce.task.io.sort.mb``
     Increase the buffer size used by the mappers during the sorting. This will
     reduce the number of spills to the disk.

  Tuning ``mapreduce.reduce.input.buffer.percent``
     If the reduce task has minimal memory requirements, then this value can be
     set to a high percentage. This means, higher amount of heap is used for
     retaining the map outputs during the reduce phase (after the shuffle phase),
     thus reducing the number of spills to disk.

  Tuning ``mapreduce.reduce.shuffle.parallelcopies``
     Number of threads used to copy map outputs to reducers.

  Compress Mapper Output
     compressing the mapper outputs, which is determined by
     ``mapreduce.map.output.compress`` (see :ref:`mapreduce_compression`).
     When compressed, less data gets written to disk and gets transferred to
     reducers.

Below are configuration parameters to consider to improve the performance:
  ``mapreduce.map.sort.spill.percent``
     This value determines the number of spills to disk.
     Default is 80%.
     It determines the threshold for the in memory buffer used by the mapper.
     When this threshold is reached, the contents of the buffer are spilled
     to disk. . It is not recommended to set it below 50%.

  ``mapreduce.task.io.sort.factor``
     Minimum number of streams to be merged at once, during sorting. Defaut is 10.
     On the reducer side, if there are 50 mapper outputs and this value is set to
     10, then there will be 5 rounds of merging
     (on an average 10 files for merge round).

  ``mapreduce.shuffle.max.threads``
     Number of worker threads for copying the map outputs to reducers.

  ``mapreduce.reduce.shuffle.input.buffer.percent``
     How much of heap should be used for storing the map output, during the
     shuffle phase in the reducer. This setting determines the amount of mapper
     output that can be held in memory, before it is spilled to disk.

  ``mapreduce.reduce.shuffle.merge.percent``
     Threshold for starting the process of merge and spilling to disk.

  ``mapreduce.reduce.merge.inmem.threshold``
     Number of map outputs needed for starting the merge process.
     When either ``shuffle.merge.percent`` or ``inmem.threshold`` is reached,
     then the map outputs are merged and spilled to disk.


Resources
=========

.. include:: /common/mapreduce/encryption-reading-resources.rst

