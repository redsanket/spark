..  _mapreduce_compression:

**************************
Data Compression in Hadoop
**************************

.. contents:: Table of Contents
  :local:
  :depth: 3

-----------

Hadoop manages very large files which are stored in HDFS and lots of data transfer among nodes in the Hadoop cluster while storing HDFS blocks or while running map reduce tasks. Data compression in Hadoop is necessary to reduce the file size which would help in reducing storage requirements as well as in reducing the data transfer across the network.


Stages and Formats
==================

`Job output` refers to the reduce output (``part-*``) files of a Map/Reduce Job or a Map output (``part-*``) files of a Map only job.
In both cases job output files are written to the HDFS.
`Intermediate Map output` in a Map/Reduce job refers to the local output files generated by the Map phase and consumed internally by the Reduce phase.

**You can compress data in Hadoop MapReduce at various stages:**

#. *Compressing input files* - You can compress the input file that will reduce storage space in HDFS. If you compress the input files then the files will be decompressed automatically when the file is processed by a MapReduce job. Determining the appropriate coded will be done using the file name extension. As example if file name extension is .snappy hadoop framework will automatically use SnappyCodec to decompress the file.
#. *Compressing the map output* - You can compress the intermediate map output. Since map output is written to disk and data from several map outputs is used by a reducer so data from map outputs is transferred to the node where reduce task is running. Thus by compressing intermediate map output you can reduce both storage space and data transfer across network. Applications can control compression of intermediate map-outputs via the ``Configuration.set(MRJobConfig.MAP_OUTPUT_COMPRESS, boolean)``
#. *Compressing output files* - You can also compress the output of a MapReduce job. Applications can control compression of job-outputs via the ``FileOutputFormat.setCompressOutput(Job, boolean)`` api.


There are many different compression formats available in Hadoop framework. Parameters that you need to look for are:

* Time it takes to compress.
* Space saving.
* Compression format is splittable or not.



.. topic:: List of available compression formats:
   :class: definitionbox

   .. glossary::

     Deflate
       (`.deflate`). It is the compression algorithm whose implementation is zlib. `Deflate` compression algorithm is also used by gzip compression tool.
     
     gzip
       (`.gzip`). It is based on Deflate compression algorithm. Gzip compression is not as fast as LZO or snappy but compresses better so space saving is more. Gzip is not splittable.

     bzip2
       (`.bz2`). Using bzip2 for compression will provide higher compression ratio but the compressing and decompressing speed is slow. Bzip2 is splittable, Bzip2Codec implements SplittableCompressionCodec interface which provides the capability to compress / de-compress a stream starting at any arbitrary position.

     Snappy
       (`.snappy`). The Snappy compressor from Google provides fast compression and decompression but compression ratio is less. Snappy is not splittable. Filename extension is `.snappy`.

     LZ4
       (`.lz4`). It has fast compression and decompression speed but compression ratio is less. LZ4 is not splittable.

Typically you should use either `gzip` or `bzip2` compression scheme to compress your job output files.
From the grid or Map/Reduce processing standpoint, the following is an important difference:

* :term:`gzip` compressed input file can not be split across multiple maps. One map instance will be created to consume single gzipped input file and thus may restrict the granularity of the map task processing.
* :term:`bzip2` compressed input file can be split across multiple maps and thus recommended for processing the data on the grid.

Compared to `bzip2`, the `snappy` is faster but it has poor compression.

Compression and Input splits
----------------------------

As you must be knowing MapReduce job calculates the number of input splits for the job and as many map tasks are launched as the count of splits. These map tasks process the data referred by input splits in parallel.

If you compress the input file using the compression format that is not splittable, then it won't be possible to read data at an arbitrary point in the stream. So the map tasks won't be able to read split data. In this scenario MapReduce won’t create input splits and whole file will be processed by one mapper which means no advantage of parallel processing and data transfer overhead too.

Let's try to clarify it with an example. If you have a 1 GB file it will be partitioned and stored as 8 data blocks in HDFS (Block size is 128 MB). MapReduce job using this file will also create 8 input splits and launch 8 mappers to process these splits in parallel.

Now, if you compress this 1 GB file using gzip (which is not splittable) then HDFS still stores the file as 8 separate blocks. As it is not possible to read data at an arbitrary point in the compressed gzip stream, MapReduce job won’t calculate input splits and launch only one map task to process all the 8 HDFS blocks. So you lose the advantage of parallel processing and there is no data locality optimization too. Even if map task is launched on the node where data for one of the block is stored data for all the other blocks has to be transferred to the node where map task is launched.

Note that compression used is Splittable or not is a factor for text files only. If you are using container file format like sequence file or Avro then splitting is supported even if the compressor used is not splittable like Snappy or Gzip.

Compression increases CPU processing
------------------------------------

There is a performance cost associated with compressing and decompressing the data. Though you save on storage and I/O activity is less but compression and decompression requires extra CPU cycles.

Though in most of the cases compressing data increases the overall job performance, ensure that you weigh the pros and cons and compare the performance gains with compressed data.

Codecs in Hadoop
================

:abbr:`Codec (compressor-decompressor)` is the implementation of a compression-decompression algorithm. In Hadoop framework there are different codec classes for different compression formats, you will use the codec class for the compression format you are using.
The codec classes in Hadoop are as follows:

* Deflate – ``org.apache.hadoop.io.compress.DefaultCodec`` or ``org.apache.hadoop.io.compress.DeflateCodec`` (`DeflateCodec` is an alias for DefaultCodec). This codec uses `zlib` compression.
* Gzip – ``org.apache.hadoop.io.compress.GzipCodec``
* Bzip2 – ``org.apache.hadoop.io.compress.Bzip2Codec``
* Snappy – ``org.apache.hadoop.io.compress.SnappyCodec``
* LZ4– ``org.apache.hadoop.io.compress.Lz4Codec``


Applications can control compression of intermediate map-outputs through ``CompressionCodec`` which is used via the ``Configuration.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, Class)`` api. Finally, ``FileOutputFormat.setOutputCompressorClass(Job, Class)`` api specifies the ``CompressionCodec`` for job-outputs.


Tutorial: Program Compressing File in bzip2 Format
==================================================


This post shows how to compress an input file in bzip2 format in Hadoop. The Java program will read input file from the local file system and copy it to HDFS in compressed bzip2 format.
Input file is large enough so that it is stored as more than one HDFS block. That way you can also see that the file is splittable or not when used in a MapReduce job. Note here that bzip2 format is splittable compression format in Hadoop.

As explained previously, there are different codec (compressor/decompressor) classes for different compression formats. Codec class for bzip2 compression format is ``org.apache.hadoop.io.compress.Bzip2Codec``.

  .. literalinclude:: /resources/code/mapreduce/BzipCompress.java
      :language: java
      :caption: Java program to compress file in bzip2 format
      :linenos:

To run this Java program in Hadoop environment export the class path where your `.class` file for the Java program resides.
Then you can run the Java program using the following command.


  .. code-block:: bash

    $ export HADOOP_CLASSPATH=$(WORK_DIR)/bin
    $ hadoop com.verizonmedia.hadoop.examples.BzipCompress

    18/04/24 10:44:05 INFO bzip2.Bzip2Factory: Successfully
      loaded & initialized native-bzip2 library system-native
    18/04/24 10:44:05 INFO compress.CodecPool: Got brand-new compressor [.bz2]

    ## Once the program is successfully executed you can check the number
    ## of HDFS blocks created by running the `hdfs fsck` command.

    $ hdfs fsck /user/out/test.bz2

The previous command should list 2 HDFS blocks.
In order to verify that MapReduce job will create input splits or not giving this compressed file `test.bz2` as input to a `wordcount` MapReduce program. Since the compression format used is :term`bzip2`, which is a splittable compression format, there should be two input splits for the job.

  .. code-block:: bash

    $ hadoop jar wc.jar WordCount /user/out/test.bz2 dft-output /user/mapout
             

    18/04/24 10:57:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
    18/04/24 10:57:11 WARN mapreduce.JobResourceUploader: Hadoop command-line
    option parsing not performed. Implement the Tool interface and execute your
    application with ToolRunner to remedy this.
    18/04/24 10:57:11 WARN mapreduce.JobResourceUploader: No job jar file set.
    User classes may not be found. See Job or Job#setJar(String).
    18/04/24 10:57:11 INFO input.FileInputFormat: Total input files to process : 1
    18/04/24 10:57:11 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library from
      the embedded binaries
    18/04/24 10:57:11 INFO mapreduce.JobSubmitter: number of splits:2


.. admonition:: Related...
   :class: readingbox

   Check the FAQ section in :ref:`mapreduce_compression_faq`

