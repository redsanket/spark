= Quick Start
:toc:
:sectanchors:
:linkattrs:

This quick start is intended to be run on one of the Yahoo gateways. We suggest 
using Axonite Red, as it is the sandbox gateway. For those wanting to create a 
launcher box (basically your own custom gateway), see link:launchers.html[Installing Pig on Your Launcher Box].

[[qs-prereq, Prerequisites]]
== Prerequisites

The prerequisites should familiarize you with the Pig interactive shell and the 
command-line program. We're going to look at a more advanced example, so you'll 
be more comfortable working with Pig before you start your own projects.

* https://yahoo.jiveon.com/docs/DOC-46641#jive_content_id_Pig[Grid Quick Start: Pig]
* https://yahoo.jiveon.com/docs/DOC-46644#jive_content_id_5_Run_Pig[Run Pig]
* http://devel.corp.yahoo.com/hue/getting_started/index.html#using-pig[Hue: Using Pig]

[[prereq-see_als, See Also]]
=== See Also

* http://hortonworks.com/hadoop-tutorial/how-to-use-basic-pig-commands/[How to Use Basic Pig Commands]
* http://pig.apache.org/docs/r0.7.0/tutorial.html#Pig+Tutorial+File[Hadoop: Pig Tutorial]

[[qs-get_data, Getting Data for Tutorials]]
== Getting Data for Tutorials

. Log on to the Axonite Red gateway: $ ssh axonite-gw.red.ygrid.yahoo.com
. We're going to use a Web service to get government data for this tutorial. 
  Use cURL to make a call to the Government Track API and save the output to a CSV file:

  [source,bash]
  ----
  $ curl "https://www.govtrack.us/api/v2/role?current=true&format=csv&role_type=senator&fields=id,title_long,party,person__firstname,person__lastname,person__gender,person__birthday,startdate,enddate,state,website" -s -S -f >us_senators.csv
  ----

. Confirm that the CSV written to `us_senators.csv` has the same headers as the following:

  ....
  id,title_long,party,person__firstname,person__lastname,person__gender,person__birthday,startdate,enddate,state,website
  268,Senator,Republican,Roy,Blunt,male,1950-01-10,2011-01-05,2017-01-03,MO,http://www.blunt.senate.gov
  317,Senator,Republican,John,Boozman,male,1950-12-10,2011-01-05,2017-01-03,AR,http://www.boozman.senate.gov
  409,Senator,Republican,Richard,Burr,male,1955-11-30,2011-01-05,2017-01-03,NC,http://www.burr.senate.gov
  1587,Senator,Republican,John,Isakson,male,1944-12-28,2011-01-05,2017-01-03,GA,http://www.isakson.senate.gov
  1801,Senator,Republican,Mark,Kirk,male,1959-09-15,2011-01-05,2017-01-03,IL,http://www.kirk.senate.gov
  2343,Senator,Republican,Jerry,Moran,male,1954-05-29,2011-01-05,2017-01-03,KS,http://www.moran.senate.gov
  2725,Senator,Republican,Robert,Portman,male,1955-12-19,2011-01-05,2017-01-03,OH,http://www.portman.senate.gov
  3478,Senator,Republican,Patrick,Toomey,male,1961-11-17,2011-01-05,2017-01-03,PA,http://www.toomey.senate.gov
  3561,Senator,Republican,David,Vitter,male,1961-05-03,2011-01-05,2017-01-03,LA,http://www.vitter.senate.gov
  3853,Senator,Democrat,Barbara,Boxer,female,1940-11-11,2011-01-05,2017-01-03,CA,http://www.boxer.senate.gov
  3953,Senator,Republican,Michael,Crapo,male,1951-05-20,2011-01-05,2017-01-03,ID,http://www.crapo.senate.gov
  ....

. Remove the first line containing the headers: `$ echo "$(tail -n +2 us_senators.csv)" > us_senators.csv`
. Copy the file to HDFS: `$ hdfs dfs -copyFromLocal us_senators.csv`


[[qs-pig_hbase, Using Pig With HBase]]
== Using Pig With HBase

[[pig_hbase-setup, Using Pig With HBase: Setting Up]]
=== Setting Up

You do not have to register HBase JAR files in your Pig script. The Pig command 
adds the HBase JAR files and the file `hbase-site.xml` to your Java `CLASSPATH` if 
the following is present in the Bash environment:

....
HBASE_PREFIX=/home/gs/hbase/current
HBASE_CONF_DIR=/home/gs/conf/hbase
....

Also, export the variable `HBASE_PREFIX`:

 export HBASE_PREFIX=/home/gs/hbase/current


NOTE: The Hadoop variable `HBASE_CONF_DIR` is generally set in the gateway environment.


[[pig_hbase-create_table, Create HBase Table]]
=== Create HBase Table 

. Get your Kerberos credentials that you'll need for accessing the Grid: `$ kinit`
. Start the HBase shell: `$ hbase shell`
. Create the Hbase database for the US Senators: `hbase> create '$USER:us_senators', 'senator_info'`

    0 row(s) in 1.1270 seconds
    => Hbase::Table - jcatera:us_senators


[NOTE]
.Deleting an HBase Table
====

. Disable the table: `hbase> disable '$USER:us_senators'`
. Drop the table: `hbase> drop '$USER:us_senators'`
====

[[pig_hbase-populate_table, Populate HBase Table With Pit]]
=== Populate HBase Table With Pig

. Start Grunt (the Pig shell): `$ pig`
. Use the `LOAD` operator and the `CSVExcelStorage` to get the 
  data from the `CSV` file into Pig. (Be sure to replace <username> with your user name.)

  ....
  grunt> raw_data = LOAD 'hdfs:/user/$USER/cust_senators.csv' USING org.apache.pig.piggybank.storage.CSVExcelStorage(',', 'YES_MULTILINE') AS
    (
       id:long,
       title:chararray,
       party:chararray,
       firstname:chararray,
       lastname:chararray,
       gender:chararray,
       birthday:chararray,
       startdate:chararray,
       enddate:chararray,
       state:chararray,
       website:chararray
    );
  ....

. Confirm that the data was consumed by Pig: `$ dump raw_data;`

   TBD: insert results

. Populate the HBase table you created with the data. Notice we're leaving out 
  the ID. The first value in the tuple will be automatically used as the row key. 
  Replace `<namespace>` with your username. See http://pig.apache.org/docs/r0.9.1/api/org/apache/pig/backend/hadoop/hbase/HBaseStorage.html[Class HBaseStorage] for more information.

  ....
  grunt> STORE raw_data INTO 'hbase://$USER:senators' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(
          'senator_info:title,
           senator_info:party,
           senator_info:firstname,
           senator_info:lastname,
           senator_info:gender,
           senator_info:birthday,
           senator_info:startdate,
           senator_info:enddate,
           senator_info:state,
           senator_info:website'
        );
  ....

. Exit Grunt: `grunt> quit;`

[[pig_hbase-confirm_data, Confirm HBase Table Has Data]]
=== Confirm HBase Table Has Data

. Start the HBase shell: `$ hbase shell`
. Scan the table: `hbase> scan '$USER:senators'`
 
  You should see the ID numbers as rows with the columns.
  ....
  ROW                            COLUMN+CELL                                                                            
  43726                         column=senator_info:birthday, timestamp=1442006981821, value=1964-11-13       
  43726                         column=senator_info:enddate, timestamp=1442006981821, value=2021-01-03                 
  43726                         column=senator_info:firstname, timestamp=1442006981821, value=Dan                      
  43726                         column=senator_info:gender, timestamp=1442006981821, value=male                        
  43726                         column=senator_info:lastname, timestamp=1442006981821, value=Sullivan                  
  43726                         column=senator_info:party, timestamp=1442006981821, value=Republican                   
  43726                         column=senator_info:startdate, timestamp=1442006981821, value=2015-01-06               
  43726                         column=senator_info:state, timestamp=1442006981821, value=AK                           
  43726                         column=senator_info:title, timestamp=1442006981821, value=Senator                      
  43726                         column=senator_info:website, timestamp=1442006981821, value=http://www.sullivan.senate.
  ....

. To get information about a particular senator, use get with one of the row keys: 
   hbase> get '$USER:senators', <row_key>

. For example, if we used the row key 43726, you would be returned the information 
  about the senator with that ID:

  ....
  COLUMN                         CELL    
  senator_info:birthday          timestamp=1442006981821, value=1964-11-13                                                                             
  senator_info:enddate          timestamp=1442006981821, value=2021-01-03                                              
  senator_info:firstname        timestamp=1442006981821, value=Dan                                                     
  senator_info:gender           timestamp=1442006981821, value=male                                                    
  senator_info:lastname         timestamp=1442006981821, value=Sullivan                                                
  senator_info:party            timestamp=1442006981821, value=Republican                                              
  senator_info:startdate        timestamp=1442006981821, value=2015-01-06                                              
  senator_info:state            timestamp=1442006981821, value=AK                                                      
  senator_info:title            timestamp=1442006981821, value=Senator                                                 
  senator_info:website          timestamp=1442006981821, value=http://www.sullivan.senate.gov  
  ....

. Exit the HBase shell: `hbase> exit;`

[[pig_hbase-load_manipulate, Using Pig to Load and Manipulate HBase Data]]
=== Using Pig to Load and Manipulate HBase Data

We're going back to Grunt to process the data we loaded into the HBase table earlier.

. Start Pig: `$ pig`
. We'll do a little reverse engineering and now load the data from the HBase table 
  we created. Notice that we're loading the table with the column cells and requesting 
  to load the row key (`-loadKey true`), so that the row key will become `'id:lang'` 
  in our Pig raw data:

  ....
  grunt> raw = LOAD 'hbase://$USER:senators'
         USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(
           'senator_info:title,
            senator_info:party,
            senator_info:firstname,
            senator_info:lastname,
            senator_info:gender,
            senator_info:birthday,
            senator_info:startdate,
            senator_info:enddate,
            senator_info:state,
            senator_info:website', '-loadKey true')
         AS (	   
              id:long,
	      title:chararray,
	      party:chararray,
	      firstname:chararray,
	      lastname:chararray,
	      gender:chararray,
              birthday:chararray,
	      startdate:chararray,
	      enddate:chararray,
	      state:chararray,
	      website:chararray
	 );
  ....

. Try dumping the data stored in raw to confirm that the HBase table was read. 
  If loaded correctly, you should see something similar to the output from when we 
  read the CSV file: `grunt> dump raw;`
 
  ....
  (43140,Senator,Republican,Ted,Cruz,male,1970-12-22,2013-01-03,2019-01-03,TX,http://www.cruz.senate.gov)
  (43123,Senator,Republican,Deb,Fischer,female,1951-03-01,2013-01-03,2019-01-03,NE,http://www.fischer.senate.gov)
  (43121,Senator,Democrat,Heidi,Heitkamp,female,1955-10-30,2013-01-03,2019-01-03,ND,http://www.heitkamp.senate.gov)
  (43112,Senator,Independent,Angus,King,male,1944-03-31,2013-01-03,2019-01-03,ME,http://www.king.senate.gov)
  (43109,Senator,Democrat,Elizabeth,Warren,female,1949-06-22,2013-01-03,2019-01-03,MA,http://www.warren.senate.gov)
  ....

. Now that we have our data, we can manipulate it with Pig. Let's get a list of 
  the female Democrat senators: 
  
  ....
  grunt> fem_dems = FILTER raw BY gender == 'female' and party == 'Democrat';

 (3853,Senator,Democrat,Barbara,Boxer,female,2011-01-05,2017-01-03,CA,http://www.boxer.senate.gov)
 (4205,Senator,Democrat,Barbara,Mikulski,female,2011-01-05,2017-01-03,MD,http://www.mikulski.senate.gov)
 (4213,Senator,Democrat,Patty,Murray,female,2011-01-05,2017-01-03,WA,http://www.murray.senate.gov)
 (42686,Senator,Democrat,Tammy,Baldwin,female,2013-01-03,2019-01-03,WI,http://www.baldwin.senate.gov)
 (42866,Senator,Democrat,Maria,Cantwell,female,2013-01-03,2019-01-03,WA,http://www.cantwell.senate.gov)
 (42868,Senator,Democrat,Dianne,Feinstein,female,2013-01-03,2019-01-03,CA,http://www.feinstein.senate.gov)
 (42871,Senator,Democrat,Debbie,Stabenow,female,2013-01-03,2019-01-03,MI,http://www.stabenow.senate.gov)
 ....

. You can get a count of the female Democratic senators now: 
   grunt> fem_dems_count = foreach (group fem_dems all) generate COUNT(fem_dems);

. Let's see how many there are: `grunt> dump fem_dems_count;`
  ....
  (14)
  ....

. Try using some of Pig's operators and functions listed in the 
  http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html[Pig Latin Reference Manual 2] 
  to further analyze the data.

. Let's quit Grunt for now: `grunt> quit;`

[[load_manipulate-see_also, Using Pig to Load and Manipulate HBase Data: See Also]]
==== See Also

* http://gethue.com/hadoop-tutorial-use-pig-and-hive-with-hbase/[Use Pig and Hive with HBase]

[[qs-pig_hive, Using Pig With Hive Through HCatalog]]
== Using Pig With Hive Through HCatalog

We took a look at how you can use Pig to load Hive data in 
https://yahoo.jiveon.com/docs/DOC-46646#jive_content_id_Loading_Data_From_HCatalog[Loading Data From HCatalog].
In this section, we're going to go into more detail, creating a Hive table and 
populating it with data, loading that data into Pig, and then manipulating the data. 

Before we start, it's important to first understand that HCatalog is a management 
layer that makes Hive metadata available to different Hadoop clients such as Pig 
and MapReduce. Without HCatalog, Pig has no way to access Hive data.

We're going to use our `senators.csv` file again, but this time with Hive and Pig.

[[pig_hive-create_table, Creating the Hive Table]]
=== Creating the Hive Table

. Start the Hive shell: `$ hive`
. Create the Hive database that will store the tables with our data using your home 
  directory (replace `<username>` with your username) to store the Hive data:
  ....
  hive> CREATE DATABASE $USER_senators LOCATION '/user/$USER/us_senators';
  ....

. Select the database you created: `hive> USE $USER_senators`
. Create the Hive external table senator_information: 

   hive> create external table if not exists senator_information(
   id int,
   title string,
   party string,
   firstname string, 
   lastname string, 
   gender string,
   birthday string,
   startdate string,
   enddate string,
   state string,
   website string) 
   ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
   stored as textfile
   location '/user/$USER/us_senators/information';

. Load the CSV data into the table: 
  [source,hive]
  ----
  hive> LOAD DATA INPATH '/user/$USER/us_senators.csv' OVERWRITE INTO TABLE senator_information;
  ----
. Use the `DESCRIBE` operator to example the Hive data structure: 
  [source,hive]
  ----
  hive> describe senator_information;
  ----

   TBD - need output
. Based on the table structure, let's select a few fields and ask for ten rows: 

   hive> select title, party, lastname from senator_information LIMIT 10;
  
  ....
  Senator	Republican	Blunt
  Senator	Republican	Boozman
  Senator	Republican	Burr
  Senator	Republican	Isakson
  Senator	Republican	Kirk
  Senator	Republican	Moran
  Senator	Republican	Portman
  Senator	Republican	Toomey
  Senator	Republican	Vitter
  Senator	Democrat	Boxer
  ....

. Let's quit Hive for now: `hive> quit;`


[[pig_hive-load_data, Load Data From Hive]]
=== Load Data From Hive

In this section, we're going to use Pig to load data from the Hive table we 
created and then use Pig to process the data.

. To use HCatalog within Grunt, you need to pass the option `-useHCatalog` to the pig program: `$ pig -useHCatalog;`
. As we did earlier, use the `HCatLoader()` function to load the data from the `senator_information` table. 

+
----
grunt> senators = LOAD '$USER_senators.senator_information' using org.apache.hive.hcatalog.pig.HCatLoader();
----
+

. Let's take a look at the raw data represented as Pig tuples:

+  
....
grunt> dump senators; 

(43595,Senator,Republican,Tom,Cotton,male,1977-05-13,2015-01-06,2021-01-03,AR,http://www.cotton.senate.gov)
(43628,Senator,Republican,Steve,Daines,male,1962-08-20,2015-01-06,2021-01-03,MT,http://www.daines.senate.gov)
(43661,Senator,Democrat,Cory,Booker,male,1969-04-27,2015-01-06,2021-01-03,NJ,http://www.booker.senate.gov)
(43726,Senator,Republican,Dan,Sullivan,male,1964-11-13,2015-01-06,2021-01-03,AK,http://www.sullivan.senate.gov)
(43727,Senator,Republican,David,Perdue,male,1949-12-10,2015-01-06,2021-01-03,GA,http://www.perdue.senate.gov)
....
+

. We can use Pig to group senators by gender and then find out the oldest member 
  of the Senate for each gender:

+
[source,pig]
-----
grunt> senators_by_gender = group senators by gender; // <!--1-->
grunt> senators_oldest_by_gender = foreach senators_by_gender generate group, MIN(senators.birthday); <!--2-->
-----
<1> Group senators by gender.
<2> Find the oldest member.
+

. Display the results:

+
----
grunt> dump senators_oldest_by_gender;

(male,1933-09-17)
(female,1933-06-22)
----
+


. We can do something a little more complicated with the data. The following Pig 
statements will sort the Senators from the oldest to youngest:

+
[source,pig]
----
grunt> senator_with_age = foreach senators generate firstname, lastname, gender, party, state, YearsBetween(CurrentTime(),ToDate(birthday)) as age; # <1>
grunt> senator_with_age = foreach senator_with_age generate gender, party, state, age, lastname, CONCAT(firstname,' ', lastname) as name; # <2>
grunt> senators_oldest_to_youngest = ORDER senator_with_age BY age DESC, lastname ASC; # <3>
grunt> senators_oldest_to_youngest = foreach senators_oldest_to_youngest generate name, age, gender, party, state; # <4>
----
<1> Iterate through senators and generate tuple with the information we want.
<2> Concatenate the first and last name.
<3> Order the results by descending age (oldest to youngest) and then by ascending last name (A-Z).
<4> Generate the results with the fields in a logical order.
+

. Display the results: 

+
----
grunt> dump senators_oldest_to_youngest;

(Dianne Feinstein,82,female,Democrat,CA)
(Charles Grassley,81,male,Republican,IA)
(Orrin Hatch,81,male,Republican,UT)
(Richard Shelby,81,male,Republican,AL)
(James Inhofe,80,male,Republican,OK)
(John McCain,79,male,Republican,AZ)
...
----
+

[[qs-pig_oozie, Using Pig With Oozie]]
== Using Pig With Oozie

In this section, we're going to create an Oozie Workflow that copies the data 
from the Government Tracker API, creates a Hive table, populates the table with 
the data, then processes the data and stores the data in two CSV files. We'll be 
creating an Oozie Workflow with Shell, Hive, Pig, and DistCp actions. 

[[pig_oozie-setup, Using Pig With Oozie: Setting Up]]
=== Setting Up

Before we set up the Workflow and write the Pig, Hive, and shell scripts, we need 
to create some directories, fetch new data and a Hive configuration file.

. Let's directory for Oozie application: `$ mkir pig_oozie`
. Change to pig_oozie and then create directories for scripts, input, and output.

+
....
$ cd pig_oozie
$ mkdir input output hive pig shell 
....
+

. Your Oozie workflow is not going to create a new database when you run it, 
  just tables. So, let's create the Hive database where you'll create tables in your workflow:

+
----
$ hive
hive> CREATE DATABASE $USER_bills LOCATION '/user/$USER/bills'; 
----
+

. Copy the file `hive-site.xml` from Cobalt Blue to your local `pig_oozie` directory:

+
....
$ hdfs dfs -copyToLocal hdfs://cobaltblue-nn1.blue.ygrid.yahoo.com:8020/user/sumeetsi/hive-site.xml pig_oozie
....
+

. Fetch the data for the Oozie application and copy it to HDFS. (We're using `sed`
to remove the header line.)

+
----
$ curl "https://www.govtrack.us/api/v2/bill?format=csv&fields=id,title,bill_type,current_status,current_status_date,link,sponsor__firstname,sponsor__lastname,sponsor__gender,sponsor_role__description,sponsor_role__party,sponsor_role__role_type" -s -S -f | sed '1d' >bills_info.csv // <1>
$ hdfs dfs -copyFromLocal /homes/$USER/bills_info.csv // <2>
----
<1> Copy the government data to a CSV file in your home directory.
<2> Copy the file to HDFS.
+

[[pig_oozie-create_config, Create Configuration]]
=== Create Configuration

The Oozie Workflow is a combination of code and configuration for that code. The 
configuration sets variables, provides credentials, organizes the execution of 
actions, and defines how to handle errors. You can also use Workflow to fork 
actions, use conditionals, and merge results, but our Workflow will be simple 
for the purpose of demonstration.

. In the `pig_oozie` directory, create the `job.properties` file with the content 
below. The Job Properties defines variables that you'll use in your `workflow.xml`. 

+
----
# General information for Oozie
nameNode=hdfs://axonitered-nn1.red.ygrid.yahoo.com:8020
jobTracker=axonitered-jt1.red.ygrid.yahoo.com:8032
queueName=default
appRoot=pig_oozie
oozie.wf.application.path=${nameNode}/user/${user.name}/${appRoot}/
appPath=${oozie.wf.application.path}
timezone=UTC
HCAT_URI=thrift://axonitered-hcat.ygrid.vip.bf1.yahoo.com:50513
HCAT_PRINCIPAL=hcat/axonitered-hcat.ygrid.vip.bf1.yahoo.com@YGRID.YAHOO.COM

# Original data for application
CSV=bills_info.csv
src_dir=${nameNode}/user/${user.name}
dest_dir=${src_dir}/${appRoot}/input

# Scripts used in Oozie actions
shellAction=shell/copy_data.sh
hiveScript=hive/create_table.hql

# Source and storage information for Hive
hiveLog=./hivelogs
dataBase=${user.name}_bills
billsTable=bills_information
databasePath=/user/jcatera/bills
dataInputFile=/user/${user.name}/${appRoot}/input/current_${CSV}

# Pig configuration.
pigScript=pig/process_hive_data.pig
csvHouse=${appPath}/output/${date}_house_bills
csvSenate=${appPath}/output/${date}_senate_bills
----
+


. The `workflow.xml` is the driver of your Oozie application. The variables you 
see in the `workflow.xml` below are defined in the `job.properties` file you just created.

+
[source,xml]
----
<workflow-app xmlns="uri:oozie:workflow:0.4" name="bills-wf">
   <credentials>
     <!-- Use HCatalog credentials for Pig/Hive. -->
     <credential name='hcatauth' type='hcat'>
       <property>
         <name>hcat.metastore.uri</name>
         <value>${HCAT_URI}</value>
       </property>
       <property>
         <name>hcat.metastore.principal</name>
         <value>${HCAT_PRINCIPAL}</value>
       </property>
    </credential>
  </credentials>
  <start to="shell-copydata"/>
  <!-- Copies the data from the local file system to HDFS -->
  <action name="shell-copydata">
    <shell xmlns="uri:oozie:shell-action:0.3">
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <configuration>
        <property>
          <name>mapred.job.queue.name</name>
          <value>${queueName}</value>
        </property>
    <property>
      <name>oozie.action.sharelib.for.distcp</name>
      <value>distcp</value>
    </property>
      </configuration>
      <exec>shell/copy_data.sh</exec>
      <argument>${CSV}</argument>
      <argument>${src_dir}</argument>
      <argument>${dest_dir}</argument>
      <file>${wf:appPath()}${shellAction}#${shellAction}</file>
      <capture-output/>
    </shell>
    <ok to="hive-create_table"/>
    <error to="kill"/>
  </action>
  <!-- Create a Hive table for the data and then populate the table. -->
  <action name='hive-create_table' cred='hcatauth'> 
   <hive xmlns="uri:oozie:hive-action:0.4">
     <job-tracker>${jobTracker}</job-tracker>
     <name-node>${nameNode}</name-node>
     <configuration>
       <property>
         <name>hive.querylog.location</name>
         <value>${hiveLog}</value>
       </property>
       <property>
         <name>mapred.job.queue.name</name>
         <value>${queueName}</value>
       </property>
       <property>
         <name>oozie.action.sharelib.for.hive</name>
         <value>hcat_current,hive_current,tez_current</value>
       </property>
     </configuration>
     <script>${hiveScript}</script>
     <param>USER=${wf:user()}</param>
     <param>HIVE_DB_PATH=${databasePath}</param>
     <param>HIVE_DB=${dataBase}</param>
     <param>HIVE_TB=${billsTable}</param>
     <param>NAMENODE=${nameNode}</param>
     <param>TIMESTAMP=${date}</param>
     <param>INPUT_DATA=${dataInputFile}</param>
     <file>${hiveScript}#${hiveScript}</file>
   </hive>
   <ok to="pig-process-data"/>
   <error to="kill"/>
  </action>
  <!-- Extract results from the Hive table and create CSV files. -->
  <action name="pig-process-data" cred='hcatauth'>
    <pig>
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <prepare>
        <delete path="${nameNode}/user/${wf:user()}/${appRoot}/output/"/>
        <mkdir path="${nameNode}/user/${wf:user()}/${appRoot}/output/"/>
      </prepare>
      <configuration>
        <property>
          <name>mapred.job.queue.name</name>
          <value>${queueName}</value>
        </property>
        <property>
          <name>mapred.compress.map.output</name>
          <value>true</value>
        </property>
        <property>
          <name>oozie.action.sharelib.for.pig</name>
          <value>hcat_current,pig_current,tez_current</value>
        </property>
      </configuration>
      <script>${pigScript}</script>
      <param>USER=${wf:user()}</param>
      <param>TIMESTAMP=${date}</param>
      <param>OUTPUT_HOUSE=${csvHouse}</param>
      <param>OUTPUT_SENATE=${csvSenate}</param>
      <file>${pigScript}#${pigScript}</file>
    </pig>
    <ok to="archive_data"/>
    <error to="kill"/>
  </action>
  <!-- Copy latest results to the archive folder. -->
  <action name="archive_data">
    <distcp xmlns="uri:oozie:distcp-action:0.2">
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <arg>${nameNode}/user/${wf:user()}/${appRoot}/output/*</arg>
      <arg>${nameNode}/user/${wf:user()}/${appRoot}/archive/</arg>
    </distcp>
    <ok to="end"/>
    <error to="kill"/>
  </action>
  <kill name="kill">
    <message>Action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
  </kill>
  <end name="end"/>
</workflow-app>
----
+


[[pig_oozie-shell_script, Shell Script]]
=== Shell Script

Create shell script `get_data.sh` in the shell directory with the code below. The 
arguments in the shell script are provided by the argument elements of the Shell 
action in the `workflow.xml`.

[source,bash]
----
#! /usr/local/bin/bash

csv_file="$1"
src_dir="$2"
dest_dir="$3"

/home/gs/hadoop/current/bin/hdfs dfs -cp -f $src_dir/$csv_file $dest_dir/current_$csv_file
----


[[pig_oozie-hive_script, Hive Script]]
=== Hive Script

Create the Hive script `hive/create_table.hql` with the following code. You might 
notice that the `TIMESTAMP` variable is declared, but not assigned a value. We're 
going to define the value when we run start the Oozie job with the `oozie` command.

[source,hive]
----
create external table if not exists ${USER}_bills.information_${TIMESTAMP} (
  id int,
  title string,
  bill_type string,
  current_status string,
  current_status_date string,
  link string,
  bsponsor__firstname string,
  sponsor__lastname string,
  sponsor__gender string,
  sponsor_role__description string,
  sponsor_role__party string,
  sponsor_role__role_type string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
stored as textfile
location "${NAMENODE}/user/${USER}/bills/${TIMESTAMP}";
LOAD DATA INPATH "${INPUT_DATA}" OVERWRITE INTO TABLE ${USER}_bills.information_${TIMESTAMP};
----

[[pig_oozie-pig_script, Pig Script]]
=== Pig Script

With the code below, create the file `pig/process_hive_data.pig`. The script is 
fairly simple: access the Hive table `information_${TIMESTAMP}` through HCatalog 
and then create two directories containing CSV results based on the bill types.

[source,pig]
----
bills = LOAD '${USER}_bills.information_${TIMESTAMP}' using org.apache.hive.hcatalog.pig.HCatLoader();
house_bills = FILTER bills BY (bill_type == 'house_bill');
senate_bills = FILTER bills BY (bill_type == 'senate_bill');
STORE house_bills INTO '${OUTPUT_HOUSE}' USING org.apache.pig.piggybank.storage.CSVExcelStorage();
STORE senate_bills INTO '${OUTPUT_SENATE}' USING org.apache.pig.piggybank.storage.CSVExcelStorage();
----

[[pig_oozie-run, Run the Oozie Job]]
=== Run the Oozie Job

. Copy the directory to HDFS: `$ hdfs dfs -put -f /user/$USER/pig_oozie /user/$USER`
. From the `pig_oozie` directory, start your Oozie application (we're defining the data variable on the command line): 
+
....
 $ oozie job -D date=`date "+%Y_%m_%d"` -config job.properties -run
....
+

. Oozie will return a job ID that will look similar to that below:

+
....
job: 0189333-150922143441638-oozie_AR-W
....
+

. Go to the Oozie Console and find your job ID in the **Job Id** column. 
Once the job completes and is successful, view the results written to ``pig_oozie/output``: 

+
....
$ hdfs dfs -cat pig_oozie/output/2015_09_28_senate_bills.csv/part*
....
+


[[pig_oozie-troubleshoot, Troubleshooting the Oozie Job]]
=== Troubleshooting the Oozie Job

If your job failed or was killed, try the following to find the issue:

. From the **Oozie Console**, click the job ID to open a dialog containing action IDs for each action. 
. Find the failed action, click the action ID. 
. From the **Action** dialog, copy the URL of the **Console URL**.
. Open the Console URL in a new browser tab. (The browser will have to connect to the Internet through the SOCKS proxy (`socks.yahoo.com`) if you're not inside the Yahoo intranet.)
. From the **Oozie Console**, click the logs link. 
. You'll be taken to the Job Tracker that has logs for `stderr`, `stdout`, and `syslog`. You can also click a link for each type of log to see the entire log file.

Also, look for common issues such as incorrect file names and script paths, undefined variables, database, or tables, and authorization issues. 

