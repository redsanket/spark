#!/bin/bash

#################################################################################
# This is a script that automates the process for backup and re-imaging a Hadoop
# cluster for the purpose of Hadoop QE. 
#################################################################################
#
# The manual instruction can be found at:
# http://twiki.corp.yahoo.com/view/Grid/SelfHelpBreakfix
#
# The script is store on github at: 
# https://git.corp.yahoo.com/HadoopQE/hadooptest/blob/master/hadooptest/scripts/clusterconfig 
# You can clone the git repository of hadooptest to your local machine by
# running the following command:
# $ yinst install git -br test
# $ cd <your local directory here you want to clone the git repository>
# $ git clone git@git.corp.yahoo.com:HadoopQE/hadooptest.git
# After clone the git repo to your local macchine, the script can be found in
# the directory <local git repo dir>/hadooptest/scripts
#
# 1) In order to update the opsdb database, you will need to be in the opsdb
# User Group 'ygrid_devtools'.
# See http://opsdb.ops.yahoo.com/user_groups.php?action=view&id=551101
#
# 2) After cluster nodes are re-imaged, if they fail to come up, particulary due
# to disk issues, file ticket against site-ops for bad nodes (one ticket per
# node as per site-ops policy) at: 
# http://portal.ops.yahoo.com:9999/srv_repRequest.php
# 
# 3) This script depends on the dist package 'expecter' for handling passwords.
# This can be installed by running: yinst install expecter 
# $ yinst ls -files expecter
# yinst: /home/y/bin/expecter                             expecter-1.0.2
#
# 4) The script can be run from a blessed machine on the corp side. For example,
# your personal linux box will work.  However, it will not work running from
# machines on the colo side (E.g. grid machines: gateway host, adm102, etc.
#
#################################################################################
# USAGE
#################################################################################
# 1) Backup the keytab and local config directory
#    $ expecter -n -v ./clusterconfig -c mallard
# 2) Update opsdb to configure the OS fields for the cluster nodes 
#    $ expecter -n -v ./clusterconfig -c mallard --update_opsdb
# 3) Display the information from opsdb for the updated cluster nodes
#    $ expecter -n -v ./clusterconfig -c mallard --show_opsdb
# 4) After a few minutes, image the cluster nodes
#    $ expecter -n -v ./clusterconfig -c mallard --image_cluster
# 5) After a few minutes, check on the status of the cluster nodes
#    $ expecter -n -v ./clusterconfig -c mallard --validate
# 6) Restore the cluster nodes with the backed up keytab and local config
#    $ expecter -n -v ./clusterconfig -c mallard --restore
# 7) Validate the restore backup files
#    $expecter -n -v ./clusterconfig -c mallard --validate_restore

# For one off nodes (e.g. after problem nodes are repaired), use the --node or
# --nodes arguments to run commands on specific node or a list of defined nodes.
#
# $ expecter -n -v ./clusterconfig -c mallard --restore --node fsbl295n19.blue.ygrid.yahoo.com
# $ expecter -n -v ./clusterconfig -c mallard --restore --nodes /tmp/nodes
#
# $ expecter -n -v ./clusterconfig -c mallard --validate --node fsbl295n19.blue.ygrid.yahoo.com
# $ expecter -n -v ./clusterconfig -c mallard --validate --nodes /tmp/nodes
#
# Override the cache directory by using the -w argument. 
# $ ./clusterconfig -w /tmp -c theoden 

ALWAYS_FETCH_NODES=0
ALWAYS_BACKUP=0
NONE_CLUSTER_MODE=0
GATEWAY_MODE=0
UPDATE_OPSDB=0
SHOW_OPSDB=0
IMAGE=0;
VALIDATE=0;
VALIDATE_RESTORE=0;
FETCH_NODES_ONLY=0;
RESTORE=0;
WORKSPACE_ROOT="$HOME"
SINGLE_HOST=""
SSH="/usr/bin/ssh -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
SCP="/usr/bin/scp -p -q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
OS_VERSION="6.4.1"
REQ_OS="6.4";

###########################################
# Function to backup cluster node content
# $1 - target host
# $2 - source dir path
# $3 - target dir path
# E.g.
# backupHost "/etc/grid-keytabs" "$WORKSPACE/$host/keytabs"
# backupHost "/home/gs/conf/local" "$WORKSPACE/$host/localconf"
###########################################
function backupHostDir {
    local host=$1
    local sourceDir=$2
    local targetDir=$3

    `/bin/mkdir -p $targetDir`
    echo "---> Backup files from cluster node '$host' in '$sourceDir' to localhost '$targetDir'";

    # create a manifest
    $SSH $host ls -l "$sourceDir/" > $targetDir/files.mf
    TAR_FILE="/tmp/files.tgz"
    set -x
    $SSH -t $host "sudo tar -zcf $TAR_FILE -C $sourceDir .";
    $SCP -p $host:$TAR_FILE $targetDir;
    set +x
}

function usage
{
    cat <<EOF
usage: $0  --cluster             |-c <cluster name> or --noneclustermode
          [--workspace           |-w <workspace directory ]
          [--always_fetch_nodes  |-a ]
          [--always_backup       |-b ]
          [--node                |-n <node name>]
          [--nodes                   <file containing list of nodes> ]
          [--update_opsdb        |-u ]
          [--show_opsdb          |-s ]
          [--image_cluster       |-i ]
          [--restore             |-r ]
          [--validate            |-v ]
          [--validate_restore        ]
          [--help                |-h ]

--cluster            Name of the cluster
--always_fetch_nodes Always fetch nodes from igor (default is false, i.e. don't re-fetch)
--always_backup      Always backup the keytab files (default is false, i.e. don't backup if it's already been done)
--noneclustermode    Run script for one off node that is not part of a cluster (e.g. gateway). Requires --node <value>
--gatewaymode        Run script for all of the flubber cluster gateways.
--node               Use a temporary alternative node (e.g. to restore a specific node after hardware fixes)
--nodes              Use temporary alternative nodes list (e.g. to restore specific nodes after hardware fixes)
--update_opsdb       Update node configuration in opsdb for cluster nodes (default is false)
--show_opsdb         Display node configurations in opsdb for cluster nodes (default is false)
--image_cluster      Image the cluster nodes (default is false)
--restore            Restore backup files (default is false)
--validate           Validate the cluster nodes (default is false)
--validate_restore   Validate the backup files for the cluster nodes have been restored (default is false)

Example:
# 1) Backup the keytab and local config directory
#    $ expecter -n -v ./clusterconfig -c mallard
# 2) Update opsdb to configure the OS fields for the cluster nodes 
#    $ expecter -n -v ./clusterconfig -c mallard --update_opsdb
# 3) Display the information from opsdb for the updated cluster nodes
#    $ expecter -n -v ./clusterconfig -c mallard --show_opsdb
# 4) After a few minutes, image the cluster nodes
#    $ expecter -n -v ./clusterconfig -c mallard --image_cluster
# 5) After a few minutes, check on the status of the cluster nodes
#    $ expecter -n -v ./clusterconfig -c mallard --validate
# 6) Restore the cluster nodes with the backed up keytab and local config
#    $ expecter -n -v ./clusterconfig -c mallard --restore
# 7) Validate the restore backup files
#    $ expecter -n -v ./clusterconfig -c mallard --validate_restore

$ expecter -n -v ./clusterconfig -c mallard
$ expecter -n -v ./clusterconfig -c mallard --update_opsdb
$ expecter -n -v ./clusterconfig -c mallard --show_opsdb
$ expecter -n -v ./clusterconfig -c mallard --image_cluster
$ expecter -n -v ./clusterconfig -c mallard --validate
$ expecter -n -v ./clusterconfig -c mallard --restore
$ expecter -n -v ./clusterconfig -c mallard --validate_restore

# expecter -n -v ./clusterconfig -c mallard --restore --node fsbl295n19.blue.ygrid.yahoo.com
# expecter -n -v ./clusterconfig -c mallard --restore --nodes /tmp/nodes
$ expecter -n -v ./clusterconfig -c mallard --validate --node fsbl295n19.blue.ygrid.yahoo.com
$ expecter -n -v ./clusterconfig -c mallard --validate --nodes /tmp/nodes
$ clusterconfig -w /tmp -c theoden 

# expecter -n -v ./clusterconfig --node gwbl2003.blue.ygrid.yahoo.com --noneclustermode
# expecter -n -v ./clusterconfig --gatewaymode

Expecter is a wrapper application that provides expect functions
$ yinst ls -files expecter
yinst: /home/y/bin/expecter                             expecter-1.0.2

EOF
}

# --fetch_nodes_only  Only fetch the cluster nodes from igor

while [ "$1" != "" ]; do
    case $1 in
        -c | --cluster )          shift
                                  CLUSTER=$1
                                  ;;
        -f | --fetch_nodes_only ) FETCH_NODES_ONLY=1
                                  ;;
        -a | --always_fetch_nodes ) ALWAYS_FETCH_NODES=1
                                  ;;
        -b | --always_backup )    ALWAYS_BACKUP=1
                                  ;;
        --noneclustermode )       NONE_CLUSTER_MODE=1
                                  ;;
        --gatewaymode )           GATEWAY_MODE=1;
                                  ;;
        -u | --update_opsdb )     UPDATE_OPSDB=1
                                  ;;
        -s | --show_opsdb )       SHOW_OPSDB=1
                                  ;;
        -i | --image_cluster )    IMAGE=1
                                  ;;
        -r | --restore )          RESTORE=1
                                  ;;
        -v | --validate )         VALIDATE=1
                                  ;;
        --validate_restore )      VALIDATE_RESTORE=1; VALIDATE=1;
                                  ;;
        -n | --node )             shift
                                  NODE_FILE=/tmp/node.$$;
                                  SINGLE_HOST=$1;
                                  echo $1 > $NODE_FILE;
                                  UPGRADE_NODES_FILE=$NODE_FILE;
                                  ;;
        --nodes )                 shift
                                  UPGRADE_NODES_FILE=$1
                                  ;;
        -w | --workspace )        shift
                                  WORKSPACE_ROOT=$1
                                  ;;
        -h | --help )             usage
                                  exit
                                  ;;
        * )                       usage
                                  exit 1
    esac
    shift
done

if [[ $NONE_CLUSTER_MODE == 1 ]]; then
    echo "WARNING: CLUSTER value is not defined. Running in none cluster, single host mode:";
    if [[ -z $SINGLE_HOST ]]; then
        echo "ERROR: Required --node value not defined for running with option --noneclustermode!!!";
        exit 1;
    fi
    WORKSPACE="$WORKSPACE_ROOT/cluster-reimage-backup/$SINGLE_HOST"
else
    if [[ $GATEWAY_MODE == 1 ]]; then
        CLUSTER="gateway";
    fi

    if [[ -z $CLUSTER ]]; then
        echo "ERROR: Required CLUSTER value not defined!!!";
        exit 1;
    fi

    WORKSPACE="$WORKSPACE_ROOT/cluster-reimage-backup/$CLUSTER"
fi

#################################################################################
# Fetch the cluster nodes
#################################################################################
if ([[ $ALWAYS_FETCH_NODES == 1 ]] || [[ $ALWAYS_BACKUP == 1 ]]); then
    set -x
    rm -rf $WORKSPACE
    set +x
fi 

set -x
/bin/mkdir -p $WORKSPACE
set +x

# Check cluster backup if CLUSTER is defined
if [[ -n $CLUSTER ]]; then
    CLUSTER_NODES="$WORKSPACE/cluster_nodes"
    if ([[ ! -f $CLUSTER_NODES ]] || [[ $ALWAYS_FETCH_NODES == 1 ]]); then
        echo "Fetch and create the cluster node list file '$CLUSTER_NODES'"
        if [[ $GATEWAY_MODE == 1 ]]; then
            touch $CLUSTER_NODES

            # file=/tmp/gateways; touch $file; for host in `igor list -roles grid_re.clusters.*.gateway`;do echo -n "$host " >> $file; igor fetch -member $host >> $file ; done; 
            # for gateway in `cut -d" " -f2 /tmp/gateways |sort -u`;do if [[ -z $gateway ]];then continue; fi; echo -n "$gateway "; grep $gateway /tmp/gateways |cut -d"." -f3|tr '\n' ' ';echo;done
            for host in `igor list -roles grid_re.clusters.*.gateway`;do 
                echo -n "$host " >> $CLUSTER_NODES ;
                igor fetch -member $host >> $CLUSTER_NODES ;
            done;
        else
            `igor fetch -members grid_re.clusters.$CLUSTER > "$CLUSTER_NODES" 2> /dev/null`
        fi
    else 
        echo "Use existing cluster node list file '$CLUSTER_NODES'"
    fi

    if [[ $GATEWAY_MODE == 1 ]]; then
        for gateway in `cut -d" " -f2 $CLUSTER_NODES |sort -u`;do 
            if [[ -z $gateway ]];then 
                continue; 
            fi;
            echo -n "$gateway --> ";
            grep $gateway $CLUSTER_NODES |cut -d"." -f3|tr '\n' ' ';
            echo;
        done
    fi   
fi 

UPGRADE_NODES_FILE=${UPGRADE_NODES_FILE:="$WORKSPACE/cluster_nodes.upgrade"}

FETCH_UPGRADE_NODES=0
if [[ ! -f $UPGRADE_NODES_FILE ]]
then 
    FETCH_UPGRADE_NODES=1;
else
    if [[ $ALWAYS_FETCH_NODES == 1 ]]
    then
        FETCH_UPGRADE_NODES=1
        echo -n "" > $UPGRADE_NODES_FILE
    fi
fi 

if [[ $FETCH_UPGRADE_NODES == 1 ]]
then
    if [[ $GATEWAY_MODE == 1 ]]; then
        NODES=`cut -d" " -f2 $CLUSTER_NODES |sort -u|sed '/^$/d'`;
    else
        NODES=`cat $CLUSTER_NODES`;
    fi

    echo "Fetch and create the upgrade node list file '$UPGRADE_NODES_FILE'"
    # Create the upgrade node file as a record in case there are no nodes that
    # need to be upgraded.
    touch $UPGRADE_NODES_FILE
    # for host in `cat $CLUSTER_NODES`;do
    for host in $NODES;do
        echo -n "$host: ";
        ver=`$SSH $host cat /etc/redhat-release`;
        CURRENT_OS_STR="current os='$ver'";
        if (! [[ $ver =~ $REQ_OS ]]); then
            echo "$CURRENT_OS_STR --> need upgrade to '$REQ_OS'"; 
            echo "$host" >> $UPGRADE_NODES_FILE
        else 
            echo "$CURRENT_OS_STR";
        fi ; 
    done
else
    echo "*****************************************************************"
    echo "Use existing upgrade node list file '$UPGRADE_NODES_FILE'"
    cat $UPGRADE_NODES_FILE
    echo "*****************************************************************"
fi

NUM_UPGRADE_NODES=`cat $UPGRADE_NODES_FILE|wc -l`

if [[ $NONE_CLUSTER_MODE == 1 ]]; then
    echo "Number of nodes to upgrade='$NUM_UPGRADE_NODES'"
elif [[ $GATEWAY_MODE == 1 ]]; then
    echo "Number of nodes to upgrade for flubber gateways='$NUM_UPGRADE_NODES'"
else
    echo "Number of nodes to upgrade for cluster '$CLUSTER'='$NUM_UPGRADE_NODES'"
fi 

#################################################################################
# Backup keytabs
#################################################################################
BU_SRC_KT_DIR="/etc/grid-keytabs"
BU_SRC_LC_DIR="/home/gs/conf/local" 
BU_TGT_KT_BSNAME="keytabs"
BU_TGT_LC_BSNAME="localconf"

BACKUP_KEYTABS=0
NUM_TGZ_FILES=`find $WORKSPACE -name \*.tgz|wc -l`;
if [[ $NUM_TGZ_FILES == 0 ]]
then 
    BACKUP_KEYTABS=1
else
    if [[ $ALWAYS_BACKUP == 1 ]]
    then
        BACKUP_KEYTABS=1
    fi
fi 

if [[ $BACKUP_KEYTABS == 1 ]];
then 
    index=0
    # LOCAL_KEYTABS_DIR="/etc/grid-keytabs";
    for host in `cat $UPGRADE_NODES_FILE`; do
        index=$((index+1))
        echo "--> Backup cluster node ($index/$NUM_UPGRADE_NODES) '$host' for cluster '$CLUSTER':";
	backupHostDir $host $BU_SRC_KT_DIR "$WORKSPACE/$host/$BU_TGT_KT_BSNAME"
	backupHostDir $host $BU_SRC_LC_DIR "$WORKSPACE/$host/$BU_TGT_LC_BSNAME"
    done
    # show tree
    echo "run /usr/bin/tree $WORKSPACE to verify files have been properly backed up"
fi

# Check backup
#    echo "run /usr/bin/tree $WORKSPACE to verify files have been properly backed up"
echo "Backed up files:"
find $WORKSPACE -type f | xargs ls -l 


#################################################################################
# Update OPSDB
#
# Requires opsdb_client
# Requires flubber_conutil_wrapper-0.1.1
#################################################################################
index=0
for host in `cat $UPGRADE_NODES_FILE`; do
    index=$((index+1))
    if [[ $UPDATE_OPSDB == 1 ]]; then
        # "sudo -u hadoopqa /home/y/bin/opsdb_update --host $host --clone=1 --os_name RHEL --os_version $OS_VERSION --os_arch 64 --modify ysa_profile=ygrid_dn "
        echo "--> Update opsdb for cluster node ($index/$NUM_UPGRADE_NODES) '$host' for cluster '$CLUSTER':";
        # update will prompt user to wait up to 30 min
        # http://ybiip1-1-grd.ops.gq1.yahoo.com:9999/wrong.pl?h=gsbl90524.blue.ygrid.yahoo.com
        # http://ybiip1-1-grd.ops.gq1.yahoo.com:9999/wrong.pl?h=$hostname
        date
        stty -echo
        $SSH -t adm102.blue.ygrid.yahoo.com "sudo -u hadoopqa /home/y/bin/opsdb_update --host $host --clone=1 --os_name RHEL --os_version $OS_VERSION --os_arch 64 --profile ygrid_dn"
        stty echo
    fi

    if [[ $SHOW_OPSDB == 1 ]]; then
        set -x 
        /home/y/bin/opsdb --getentry ysa_os_name,ysa_os_version,ysa_os_arch,ysa_profile --host $host
        set +x
    fi
done

#################################################################################
# IMAGE THE HOSTS
#################################################################################
if [[ $IMAGE == 1 ]]; then
    index=0
    for host in `cat $UPGRADE_NODES_FILE`; do
        index=$((index+1))
        echo "--> Reboot cluster node ($index/$NUM_UPGRADE_NODES) '$host' for cluster '$CLUSTER':";
        stty -echo
        set -x
        $SSH -t adm102.blue.ygrid.yahoo.com sudo -u hadoopqa /home/y/bin/flubber_conutil -m reboot $host

        # If console is not working
        # $SSH -t $host "sudo reboot"

        set +x
        stty echo
    done
fi

#################################################################################
# VALIDATE THE HOSTS
#################################################################################
if [[ $VALIDATE == 1 ]]; then
    index=0
    for host in `cat $UPGRADE_NODES_FILE`; do
        index=$((index+1))
        echo -n "--> Validate node ($index/$NUM_UPGRADE_NODES) '$host': ";

        /usr/bin/nc -w 5 -x socks-ssh.corp.yahoo.com:1080 $host 22 > /dev/null
        rc=$?
        if [[ rc -eq 0 ]]; then

            ver=`$SSH $host cat /etc/redhat-release 2>/dev/null`;
            CURRENT_OS_STR="current os='$ver'";
            if (! [[ $ver =~ $REQ_OS ]]); then
                echo "Host is up: $CURRENT_OS_STR --> need upgrade to '$REQ_OS'"; 
            else 
                echo "Host is up: $CURRENT_OS_STR";

                # Validate that the back up files are restored properly.
                # Compare it against the manifest file that was saged originaly.
                if [[ $VALIDATE_RESTORE == 1 ]]; then

                    # Validate keytab files
                    HOST_BACKUP_DIR="$WORKSPACE/$host/$BU_TGT_KT_BSNAME"
                    $SSH $host ls -l $BU_SRC_KT_DIR/ > $HOST_BACKUP_DIR/files.actual
                    DIFF=`diff -b $HOST_BACKUP_DIR/files.mf $HOST_BACKUP_DIR/files.actual`
                    if [[ -n $DIFF ]]; then
                        echo "Expected and actual keytab files differs on host $host: $DIFF"
                    else 
                        echo "Expected and actual keytab files match on host $host."
                    fi

                    # Validate local conf files
                    HOST_BACKUP_DIR="$WORKSPACE/$host/$BU_TGT_LC_BSNAME"
                    # LOCAL_CONF_DIR=$BU_SRC_LC_DIR;
                    $SSH $host ls -l $BU_SRC_LC_DIR/ > $HOST_BACKUP_DIR/files.actual
                    DIFF=`diff -b $HOST_BACKUP_DIR/files.mf $HOST_BACKUP_DIR/files.actual`
                    if [[ -n $DIFF ]]; then
                        echo "Expected and actual local conf files differs on host $host: $DIFF"
                    else 
                        echo "Expected and actual local conf files match on host $host."
                    fi
                fi;
            fi ; 
        else
            echo "Host is down. To debug, run: ssh -t adm102.blue.ygrid.yahoo.com sudo -u hadoopqa /home/y/bin/flubber_conutil -m console $host";
            echo "If problem persists (e.g. bad disk), file ticket against site-ops at http://portal.ops.yahoo.com:9999/srv_repRequest.php"
        fi

    # /usr/bin/nc -w 5 -x socks-ssh.corp.yahoo.com:1080 fsbl295n32.blue.ygrid.yahoo.com 22
    # nc: connection failed, SOCKS error 5
    # [philips@noiseround:4517 ~]$ nc -w 5 -x socks-ssh.corp.yahoo.com:1080 fsbl294n32.blue.ygrid.yahoo.com 22
    # SSH-2.0-OpenSSH_5.3
    done
fi

#################################################################################
# Restore keytabs
#################################################################################
if [[ $RESTORE == 1 ]];
then
    index=0
    for host in `cat $UPGRADE_NODES_FILE`; do
        index=$((index+1))
        echo "--> Restore cluster node ($index/$NUM_UPGRADE_NODES) '$host' for cluster '$CLUSTER':";
        HOST_BACKUP_DIR="$WORKSPACE/$host/$BU_TGT_KT_BSNAME"
        HOST_BACKUP_DIR2="$WORKSPACE/$host/$BU_TGT_LC_BSNAME"
        if [[ -d $HOST_BACKUP_DIR ]]; then
            # check if the host is accessible
            /usr/bin/nc -w 5 -x socks-ssh.corp.yahoo.com:1080 $host 22 > /dev/null
            rc=$?
            if [[ rc -eq 0 ]]; then
                echo "---> Restore keytab files for host '$host'"
                BU_TAR_FILE=$HOST_BACKUP_DIR/files.tgz
                if [[ -f $BU_TAR_FILE ]]; then
                    set -x
                    $SCP -p $BU_TAR_FILE $host:/tmp;
                    $SSH -t $host "sudo /bin/gtar fx /tmp/files.tgz -C $BU_SRC_KT_DIR";
                    $SSH -t $host ls -l $BU_SRC_KT_DIR;
                    set +x
                else 
                    echo "WARNING: keytab backup file '$BU_TAR_FILE' does not exists!!!!"
                fi
 
                echo "---> Restore local conf files for host '$host'"
                BU_TAR_FILE=$HOST_BACKUP_DIR2/files.tgz
                if [[ -f $BU_TAR_FILE ]]; then
                    set -x
                    $SCP -p $BU_TAR_FILE $host:/tmp;
                    $SSH -t $host "sudo /bin/gtar fx /tmp/files.tgz -C $BU_SRC_LC_DIR";
                    $SSH -t $host ls -l $BU_SRC_LC_DIR;
                    set +x
                 else
                    echo "WARNING: local conf backup file '$BU_TAR_FILE' does not exists!!!!"
                 fi

                # Fix the health check issue. This should be commented out after ticket is resolved. 
                # Bugzilla Ticket 6481091 - mapreqa user needs whitelist in health check scripts
                WORKAROUND=1
                if [[ $WORKAROUND -eq 1 ]]; then
                    echo "---> Fix health check script for host '$host'"
                    HC_SCRIPT="/usr/local/libexec/nm_health_check"
                    set -x
                    $SSH -t $host "sudo perl -pi -e \"s/'mapred'/'mapredqa'/g\" $HC_SCRIPT"
                    set +x
                fi
            fi
        else 
           echo "Backup directory '$HOST_BACKUP_DIR' does not exist!!!"
        fi
    done
fi

# Clean up temporary file
if [[ -f $NODE_FILE ]]; then 
    set -x
    /bin/rm $NODE_FILE
    set +x
fi
