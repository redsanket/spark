#!/usr/bin/env perl

use strict;
use warnings;
use FindBin qw($Bin $Script);
use Test::More;
use File::Copy;
use File::Find::Rule;
use File::Basename;
use Sys::Hostname;

my %options=();

#
# Setup shell environment on a remote host
#

sub usage {
    my ($err_msg) = @_;
    print STDERR << "EOF";

  Usage: $Script -c cluster <option1> <values1> <option2> <values2> ...
-----------------------------------------------------------------------------------
The options
        -c|--cluster    <cluster name> : cluster name
        [ -f|           <conf file>         ] : hadooptest configuration file
        [ -j|--java                         ] : run tests via java directly instead of via maven
        [ -m|--mvn                          ] : run tests via maven instead of via java directly
        [ -gdm                              ] : Run GDM tests without Hadoop being deployed to the node or cluster under test
        [ -gdmVersion                       ] : Run GDM tests with specified version ( since gdm 6.4 has a dynamic way of loading hadoop jars )
        [ -storm                            ] : Run with a Storm environment
        [ -storm090                         ] : Run with a Storm-0.9.0 environment
        [ -stormconf    <conf file>         ] : Specify the full path to a yaml configuration file describing the Storm cluster nodes
        [ -spark                            ] : Install and setup a Spark environment on the gateway prior to running tests
        [ -spark12                          ] : Install and setup a Spark environment on the gateway prior to running tests
        [ -spark13                          ] : Install and setup a Spark environment on the gateway prior to running tests
        [ -hadoop26                         ] : use Hadoop dependencies for Hadoop 2.6 (Hadoop 2.5 dependencies are default)
        [ -hadoop27                         ] : use Hadoop dependencies for Hadoop 2.7 (Hadoop 2.5 dependencies are default)
        [ -n|--nopasswd                     ] : no password prompt
        [ -w|workspace  <workspace>         ] : workspace
        [ -a|--adm_box                      ] : The admin box to be used by the framework (adm103.blue.ygrid.yahoo.com is the default)
        [ -nk                               ] : No kinit
        [ -h|--help                         ] : help

Pass Through options
        [ -P<profile>                       ] : maven profile
        [ -Pclover -Djava.awt.headless=true ] : activate clover profile
        [ -X                                ] : maven debug output
        [ -Dthread.count=<threadcount>      ] : maven thread count
        [ -Dtest=<test>...                  ] : test suite name(s). use delimitor comma for mvn, and space for java

Example:
\$ run_hadooptest --cluster theoden
\$ run_hadooptest --cluster theoden -Dtest=TestSleepJobCLI
\$ run_hadooptest --cluster theoden --mvn --nopasswd -w `pwd` -t TestEndToEndPipes
\$ run_hadooptest --cluster theoden --mvn -Dtest=TestSleepJobCLI
\$ run_hadooptest --cluster theoden --mvn -f /homes/hadoopqa/hadooptest.conf -Dtest=TestSleepJobCLI
\$ run_hadooptest -c theoden -j -n -Dtest=hadooptest.regression.TestVersion
\$ run_hadooptest -c theoden -m -P jacoco -Dtest=TestVersion
\$ run_hadooptest -c theoden -m -P clover -Djava.awt.headless=true -Dtest=TestVersion

EOF
    die($err_msg) if ($err_msg);
    exit 0;
}

sub execute {
    my ($command) = @_;
    note($command);
    system($command);
}

my $cluster;
my $conf = glob("~/hadooptest.conf");
my $use_mvn = 1;
my $gdm = 0;
my $gdmVersion = 0;
my $storm = 0;
my $storm090 = 0;
my $stormconf;
my $spark = 0;
my $tez = 0;
my $hadoop26 = 0;
my $hadoop27 = 0;
my $pig = 0;
my $spark12 = 0;
my $spark13 = 0;
my $workspace = "$Bin/..";
my $adm_box = "adm103.blue.ygrid.yahoo.com";
my $nk = 0;
my $username = getpwuid($<);
my $nopasswd = ($username eq "hadoopqa") ? 1 : 0;
my $test;
my $gridName;
my $timeStamp;

#
# Command line options processing
#
use Getopt::Long;
&Getopt::Long::Configure( 'pass_through');
my $result =
GetOptions(\%options,
    "cluster|c=s"        => \$cluster,
    "conf|f=s"           => \$conf,
    "mvn|m"              => sub { $use_mvn = 1 },
    "java|j"             => sub { $use_mvn = 0 },
    "gdm"                => \$gdm,
    "gdmVersion"          => \$gdmVersion,
    "tez"                => \$tez,
    "hadoop26"           => \$hadoop26,
    "hadoop27"           => \$hadoop27,
    "pig"                => \$pig,
    "spark"              => \$spark,
    "spark12"            => \$spark12,
    "spark13"            => \$spark13,
    "storm"              => \$storm,
    "storm090"           => \$storm090,
    "stormconf|y=s"      => \$stormconf,
    "nopasswd|n"         => \$nopasswd,
    "workspace|w=s"      => \$workspace,
    "adm_box|a=s"        => \$adm_box,
    "nk"                 => \$nk,
    "test|t=s"           => \$test,
    "help|h|?"
    ) or usage(1);
usage() if $options{help};
usage("Invalid arguments!!!") if (!$result);
usage("ERROR: Required cluster value not defined!!!") if (!defined($cluster));

note("cluster='$cluster'");
note("conf = $conf");
note("workspace='$workspace'");
note("adm box = $adm_box");
note("use_mvn='$use_mvn'");

my @tests = ($test) ? split(",", $test) :
    (
     "hadooptest.regression.TestVersion",
     "hadooptest.regression.yarn.TestSleepJobAPI"
    );

my @classpath= (
    "/home/y/lib/jars/junit.jar",
    "/home/gs/gridre/yroot.$cluster/conf/hadoop/",
    "/tmp/RandomWriter.jar",
    "/home/gs/tez/current/",
    "/home/gs/tez/current/lib" );

unless ($use_mvn) {
    push(@classpath, "$workspace/target/hadooptest-1.0-SNAPSHOT.jar");
    push(@classpath, "$workspace/target/hadooptest-1.0-SNAPSHOT-tests.jar");
}

my $hadoop_install="/home/gs/gridre/yroot.$cluster";
my @hadoop_jar_paths = (
    "$hadoop_install/share/hadoop/share/hadoop/common/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/common",
    "$hadoop_install/share/hadoop-*/share/hadoop/common/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/hdfs",
    "$hadoop_install/share/hadoop-*/share/hadoop/hdfs/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/yarn",
    "$hadoop_install/share/hadoop-*/share/hadoop/yarn/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/mapreduce",
    "$hadoop_install/share/hadoop-*/share/hadoop/mapreduce/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/hdfs"
    );

foreach my $path (@hadoop_jar_paths) {
    push(@classpath, `echo -en $path`.'/*');
};

note("CLASSPATH=".join(":",@classpath));

if($storm||$storm090) {
	my $host = hostname;
	my $host_prefix = substr($host, 0, index($host, '.'));
	if (not $nk) {
	    execute("stty -echo; kinit -kt /homes/hadoopqa/hadoopqa.dev.headless.keytab hadoopqa\@DEV.YGRID.YAHOO.COM; stty echo");}
}
else {
	execute("stty -echo; kinit $username\@DS.CORP.YAHOO.COM; stty echo")
    	unless ($nopasswd);
}

# Temporarily removing help:active-profiles as the maven-help-plugin is not
# resolving from an offline local repository.  It keeps trying to download
# the metadata from a versionless instance of the plugin.
# my $pom_opt="-f pom.xml help:active-profiles";
my $pom_opt="-f pom.xml";
my $mvn_settings_opt="-gs $workspace/resources/yjava_maven/settings.xml.gw ";
my $java_lib_path="-Djava.library.path=/home/gs/gridre/yroot.$cluster/share/hadoop/lib/native/Linux-amd64-64/";

# Create the local repo directory on /grid/0/ and set the maven.repo.local
# option.  This is necessary so we don't store the maven local repo on the
# /homes NFS mount by default, which causes performance degradation,
# especially on OpenStack.
execute("mkdir /grid/0/tmp/htf_mvn_repo");
my $dependencies_opt = "-Dmaven.repo.local=/grid/0/tmp/htf_mvn_repo";
if ($storm) {
	# When Storm is used, this profile must be called before profile-ci
	# or any other profile.  If another profile is used first, the
	# dependency jars from that profile might have Storm configuration
	# or classes that will upset the classpath assembled by maven,
	# and can lead to unexpected configuration or classes ending up in
	# the classpath before the necessary Storm support.  By putting
	# profile-storm first in the maven profile order, it ensures
	# that Storm API calls and configuration will be using the
	# deployed Storm jars. -->
    $dependencies_opt .= " -Pprofile-storm -Pprofile-all";
}
elsif ($storm090) {
	# When Storm is used, this profile must be called before profile-ci
	# or any other profile.  If another profile is used first, the
	# dependency jars from that profile might have Storm configuration
	# or classes that will upset the classpath assembled by maven,
	# and can lead to unexpected configuration or classes ending up in
	# the classpath before the necessary Storm support.  By putting
	# profile-storm first in the maven profile order, it ensures
	# that Storm API calls and configuration will be using the
	# deployed Storm jars. -->
    $dependencies_opt .= " -Pprofile-storm-090 -Pprofile-all";
}

elsif ($spark12) {
	# When Spark is used, its the same as Hadoop but here we specify
	# the spark version to override the default so we can support
	# multiple versions at once.
    if ($hadoop26) {
      $dependencies_opt .= " -Dspark.hadoop.version=2.6.0.1.1411101121 -Dspark.version=1.2.0.0 -Dspark.version.full=1.2.0.0_2.6.0.1.1411101121_1412171547 -Pprofile-hadoop -Pprofile-ci -Pprofile-all";
    } else {
      $dependencies_opt .= " -Dspark.hadoop.version=2.5.0.8.1411070359 -Dspark.version=1.2.0.0 -Dspark.version.full=1.2.0.0_2.5.0.8.1411070359_1412171833 -Pprofile-hadoop -Pprofile-ci -Pprofile-all";
    }
}
elsif ($spark13) {
    $dependencies_opt .= " -Dspark.hadoop.version=2.6.0.6.1502061521 -Dspark.version=1.3.1.0 -Dspark.version.full=1.3.1.0_2.6.0.6.1502061521_1504272233 -Pprofile-hadoop -Pprofile-ci -Pprofile-all";
}
elsif ($tez){
    $dependencies_opt .= " -Pprofile-tez -Pprofile-hadoop-2-6-plus -Pprofile-ci -Pprofile-all";
}

elsif ($hadoop27) {
    $dependencies_opt .= " -Pprofile-hadoop-2-7 -Pprofile-ci -Pprofile-all";
}

elsif ($hadoop26) {
    $dependencies_opt .= " -Pprofile-hadoop-2-6-plus -Pprofile-ci -Pprofile-all";
}


else {

	# When Hadoop is used, this profile must be called before profile-ci
	# or any other profile.  If another profile is used first, the
	# dependency jars from that profile might have Hadoop configuration
	# or classes that will upset the classpath assembled by maven,
	# and can lead to unexpected configuration or classes ending up in
	# the classpath before the necessary Hadoop support.  By putting
	# profile-hadoop first in the maven profile order, it ensures
	# that Hadoop API calls and configuration will be using the
	# deployed Hadoop jars. -->
    $dependencies_opt .= " -Pprofile-hadoop -Pprofile-ci -Pprofile-all";
}

my $stormClusterConf = "";
if (-e "$stormconf") {
   	$stormClusterConf = "-DSTORM_CLUSTER_CONF=$stormconf";
}

my @common_args = (
    "-DCLUSTER_NAME=$cluster",
    "-DWORKSPACE=$workspace",
    "-DADM_BOX=$adm_box",
    "-Dhadooptest.config=$conf",
    $stormClusterConf,
    $dependencies_opt,
    join(" ", @ARGV)
    );

my $test_opt = ($use_mvn) ?
    "-Dtest=".join(",",@tests) :
    join(" ", @tests);

# TODO: investigate merging this in build.xml.
if ( grep( /^jacoco$/, @ARGV ) ) {
    # raw results directory
    my $jacoco_result_dir="/homes/hadoopqa/jacoco/results/latest";
    execute("rm -f $jacoco_result_dir/*");

    # final results directory
    my $jacoco_result_file="$workspace/target/jacoco.exec";
    execute("rm -f $jacoco_result_file");

    # misc results directory:
    # $jacoco_result_dir="$workspace/data/exec";
    # execute("rm -f $jacoco_result_dir/*");
    # $jacoco_result_dir="$workspace/data/exec/jacoco.exec";
    # execute("rm -f $jacoco_result_dir");
}

# If the user has provided the -spark option to the script, install the Spark
# corresponding spark package so that we can run tests against the Spark jars.
if ($spark) {
    execute("yinst i yspark_yarn -tag yspark_yarn_1_1_latest_hadoop2 -live -downgrade -br quarantine");
    # Hack to support multiple versions of spark. Remove the version(s) that won't compile.
    # Note that once you run this, you have to re-copy files over to switch to use the other version.
    execute("WORKSPACE=$workspace rm -rf $workspace/htf-common/src/test/scala/hadooptest/spark/regression/spark1_2");
} elsif ($spark12) {
    if ($hadoop26) {
      execute("yinst i yspark_yarn -tag yspark_yarn_1_2_latest_hadoop2 -live -downgrade -br quarantine");
    } else {
      execute("yinst i yspark_yarn -tag yspark_yarn_1_2_latest_hadoop2.5 -live -downgrade -br quarantine");
    }
} elsif ($spark13) {
    execute("yinst i yspark_yarn -tag yspark_yarn_1_3_latest_hadoop2 -live -downgrade -br quarantine");
} else {
    execute("WORKSPACE=$workspace rm -rf $workspace/htf-common/src/test/scala/hadooptest/spark/regression/spark1_2");
}

# If the user has provided the -pig option to the script, install the Pig
# dependencies, for ABF feed, that are needed for the Pig scripts,
# invoked from within HTF. This does not deploy actual Pig package, because
# the Jenkins jobs are expected to care for that. This just cares for an internal
# HTF test dependency.
if ($pig) {
    execute("yinst i fetl_base_feed_jar");
    execute("yinst i fetl_projector");
    execute("yinst i htf_pig_QE_data -br current");
}

my $hadoop_version;
my $cmd = "ls -l /home/gs/gridre/yroot.$cluster/share/hadoop|cut -d'>' -f2|cut -d'-' -f2";
note($cmd);
chomp($hadoop_version = `$cmd`);

my $tez_version;
if ($tez){
  my $cmd = "ls -l /home/gs/tez/current/tez-api-*.jar | sed -e's/  */ /g' | cut -d' ' -f9| cut -d'-' -f3| rev | cut -d. -f2- | rev";
  note($cmd);
  chomp($tez_version = `$cmd`);
}
note("Tez version = '$tez_version'");

# Fix for GDM to init the hadoop version since it won't be installed and we
# expect hadoop 2 compatibility
if ($gdm) {
   $hadoop_version = "2.0";
}

note("Hadoop version = '$hadoop_version'");
if ($use_mvn) {
    # RUN TESTS VIA MAVEN
    execute("/usr/local/bin/yinst install yjava_maven -br test -yes") unless (-e "/home/y/bin/mvn");

    unless ($storm||$storm090) {
        $pom_opt .= " -Pprofile-hadoop";
    }

    my $hadoop_share = "/home/gs/gridre/yroot.$cluster/share/hadoop-$hadoop_version/share/hadoop/";
    if ($gdm) {
        # If the -gdm option is specified to the script, we want to
        # run with the gdm-deployed Hadoop 2.0 jars, instead of relying
        # on any cluster deployment of the Hadoop jar
        $hadoop_version = "2.0.5.0.1306301601";
        
        # use -gdmVersion to run gdm testcases against GDM 6.3 or before that. 
        if ($gdmVersion) {
        	print "==================================================================== \n ";
        	my $base_dir = "/grid/0/yroot/var/yroots/console/home/y/libexec/prod_hadoop_versions/";
        	my $find_rule = File::Find::Rule->new;
        	$find_rule->maxdepth(1);

 			# Only return directories
 			$find_rule->directory;
 			
 			# Apply the rule and retrieve the subdirectories, which should be the
        	# directories named by Hadoop version.
        	my @sub_dirs = $find_rule->in($base_dir);
 			         	
        	my @basedirs;
        	foreach my $dirname (@sub_dirs) {
        		 push @basedirs, basename($dirname);
        	}
        	
        	# inverse alphabetical sort the names so most recent version comes first
        	foreach my $basedir (@basedirs) {
        		if ($basedir =~ m/^2\.(.*)/) {
        			 print "\n\nSetting GDM Hadoop dependencies to Hadoop version $basedir\n\n";
        			 $hadoop_version = $basedir;
        			 $hadoop_share = "/grid/0/yroot/var/yroots/console/home/y/libexec/prod_hadoop_versions/$hadoop_version/share/hadoop-$hadoop_version/share/hadoop/";
        			 last;
        		}
        	}
        } else {
        	$hadoop_share = "/grid/0/yroot/var/yroots/console/home/y/libexec/prod_hadoop_versions/$hadoop_version/share/hadoop-$hadoop_version/share/hadoop/";

        	# Set up a rule to use to search the GDM Hadoop jars base dir
        	my $base_dir = "/grid/0/yroot/var/yroots/console/home/y/libexec/gdm_hadoop_configs/";
        	my $find_rule = File::Find::Rule->new;

        	# Do not descend past the first level
        	$find_rule->maxdepth(2);

        	# Only return directories
        	$find_rule->directory;

        	# Apply the rule and retrieve the subdirectories, which should be the
        	# directories named by Hadoop version.
        	my @sub_dirs = $find_rule->in($base_dir);

        	# Print out the name of each directory on its own line
        	print "\nGDM Hadoop dependency directories are:\n";
        	print join("\n", @sub_dirs);
        	print "\n\n";

	        # Get the individual directory version names
    	    my @basedirs;
		
			# select only grid folder where GDM hadoop config exists        
        	foreach my $dirname (@sub_dirs) {
        		if ( rindex($dirname,"bin") == -1 && rindex($dirname,"local") == -1 && rindex($dirname,"hive-site") == -1 )  {
        	    	push @basedirs , $dirname;
        		}
        	}
        
        	print "\n ------- Display all the grids installed by one node deployment ----------------:\n";
        	print join("\n",@basedirs);
        	print "\n\n";
        
        	# Get the last directory which is representing the grid name
        	my $last_element = $basedirs[@basedirs -1];
        	print "last element = $last_element\n";
        
        	my $finalFolder = "$last_element/hadoopcoretree/share/";
        	print "finalFolder =  $finalFolder\n" ;
        
        	# Get all the directories under hadoopcoretree/share/
        	$find_rule->maxdepth(1);
        	my @subDirs = $find_rule->in($finalFolder);
        	print join("\n",@subDirs);
        	print "\n\n";
        
        	# Navigate hadoopcoretree/share/ directory and get the hadoop version and hadoop shared directory 
        	foreach my $subDir (@subDirs) {
        		my $indexOf = rindex($subDir,"hadoop-");
        			if ( $indexOf > 0) {
	        	    	my $result = substr( $subDir ,$indexOf + length("hadoop-"), length($subDir));
	        			$hadoop_version = $result;
	        			$hadoop_share = "$subDir/share/hadoop/";
	        			print "hadoop version = $hadoop_version \n";
	        			print "\n hadoop_share = $hadoop_share \n";
        			}
        	}
        }
        
    }

    # Call the script that gets a number of required system dependencies
    # through "yinst fetch".
    if ($spark12) {
      if ($hadoop26) {
        execute("WORKSPACE=$workspace $workspace/scripts/ci_install_jar spark12hadoop26");
      } else {
        execute("WORKSPACE=$workspace $workspace/scripts/ci_install_jar spark12");
      }
    } elsif ($spark13) {
      execute("WORKSPACE=$workspace $workspace/scripts/ci_install_jar spark13");
    } else {
      execute("WORKSPACE=$workspace $workspace/scripts/ci_install_jar");
    }

    if ($storm||$storm090) {
        # Clean the project and build the jars so that we can have a test
        # jar with dependencies for Storm topologies.
        execute("HTF_WORKSPACE=$workspace /home/y/bin/mvn $pom_opt $mvn_settings_opt clean package -DskipTests $dependencies_opt");
    }
    else {
        # Just clean the project.
    	execute("CLUSTER=$cluster " .
            "HADOOP_VERSION=$hadoop_version " .
            "HADOOP_INSTALL=$hadoop_install " .
            "HADOOP_SHARE=$hadoop_share " .
            "HTF_WORKSPACE=$workspace " .
            "TEZ_VERSION=$tez_version " .
            "/home/y/bin/mvn $pom_opt $mvn_settings_opt clean package -DskipTests $dependencies_opt");
    }

    ############################################################################
    # Execute the tests.
    ############################################################################
    execute("CLUSTER=$cluster ".
            "HADOOP_VERSION=$hadoop_version " .
            "HADOOP_INSTALL=$hadoop_install " .
            "HADOOP_SHARE=$hadoop_share " .
            "HTF_WORKSPACE=$workspace " .
            "TEZ_VERSION=$tez_version " .
            "HADOOP_CONF_DIR=/home/gs/gridre/yroot." . $cluster . "/conf/hadoop " .
            "HADOOP_HOME=/home/gs/gridre/yroot." . $cluster . "/share/hadoop " .
            "HADOOP_PREFIX=/home/gs/gridre/yroot." . $cluster. "/share/hadoop " .
            "/home/y/bin/mvn $pom_opt $mvn_settings_opt test -DfailIfNoTests=false ".
            join(" ", @common_args) . " " .
            $test_opt);

} else {

    # Need to compile the hadooptest jar if it doesn't exist.
    unless (-e "$workspace/target/hadooptest-1.0-SNAPSHOT.jar") {
        execute("/home/y/bin/mvn clean package -DskipTests");
    }

    # RUN TESTS VIA JAVA
    execute("/home/y/bin/java -cp ".
            join(":",@classpath)." ".
            join(" ",@common_args)." ".
            "$java_lib_path ".
            "org.junit.runner.JUnitCore ".
            $test_opt);
}

# Write test results in TAP format to /homes/hadoopqa

# Copy the finger print file if it exists (so this test execution can be tied
# with other related Jenkins jobs.
# E.g "/home/y/var/builds/workspace/NightlyHadoopQEAutomation-23
my $result_root_dir = "/homes/hadoopqa/hudson_artifacts/";
$result_root_dir .= ($hadoop_version =~ /^0.23/) ? "hudson_artifacts-0.23" : "hudson_artifacts-2.0";
my $fingerprint = $result_root_dir."/artifacts.stamp";
if (-e $fingerprint) {
    note("copy fingerprint file '$fingerprint' to workspace '$workspace':");
    copy($fingerprint,"$workspace/artifacts.stamp");
} else {
    note("Found no upstream fingerprint file '$fingerprint' to copy to workspace '$workspace':");
}
