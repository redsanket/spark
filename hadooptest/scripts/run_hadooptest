#!/usr/bin/env perl

use strict;
use warnings;
use FindBin qw($Bin $Script);
use Test::More;
use File::Copy;
use File::Find::Rule;
use File::Basename;
use Sys::Hostname;

my %options=();
my ($cmd, $hadoop_version, $tez_version);

#
# Setup shell environment on a remote host
#

sub usage {
    my ($err_msg) = @_;
    print STDERR << "EOF";

  Usage: $Script -c cluster <option1> <values1> <option2> <values2> ...
-----------------------------------------------------------------------------------
The options
        -c|--cluster    <cluster name> : cluster name
        [ -f|           <conf file>         ] : hadooptest configuration file
        [ -j|--java                         ] : run tests via java directly instead of via maven
        [ -m|--mvn                          ] : run tests via maven instead of via java directly
        [ -gdm                              ] : Run GDM tests without Hadoop being deployed to the node or cluster under test
        [ -gdmVersion                       ] : Run GDM tests with specified version ( since gdm 6.4 has a dynamic way of loading hadoop jars )
        [ -storm                            ] : Run with a Storm environment
        [ -storm090                         ] : Run with a Storm-0.9.0 environment
        [ -stormconf    <conf file>         ] : Specify the full path to a yaml configuration file describing the Storm cluster nodes
        [ -spark                            ] : Run with an environment which has Spark setup same as the gateways with specific dependencies
        [ -spark14                          ] : Install and setup a Spark environment on the gateway prior to running tests
        [ -spark15                          ] : Install and setup a Spark environment on the gateway prior to running tests
        [ -sparktag    <dist tag>           ] : Use the specified dist tag for spark installation on gateway
        [ -sparkver    <version>            ] : Install and setup a specified version of spark (for building htf)
        [ -sparkbranch <branch>             ] : Use the specified dist branch to install spark (for dependency fetch)
        [ -sparkhadoopver <hadoopversion>   ] : Use the specified version of hadoop with spark (for building htf)
        [ -sparkfullver <full version>      ] : Use the specified full version of spark (for building htf and dependency fetch))
        [ -n|--nopasswd                     ] : no password prompt
        [ -w|workspace  <workspace>         ] : workspace
        [ -a|--adm_box                      ] : The admin box to be used by the framework (devadm102.blue.ygrid.yahoo.com is the default)
        [ -nk                               ] : No kinit
        [ -h|--help                         ] : help

Pass Through options
        [ -P<profile>                       ] : maven profile
        [ -Pclover -Djava.awt.headless=true ] : activate clover profile
        [ -X                                ] : maven debug output
        [ -Dthread.count=<threadcount>      ] : maven thread count
        [ -Dtest=<test>...                  ] : test suite name(s). use delimitor comma for mvn, and space for java

Example:
\$ run_hadooptest --cluster theoden
\$ run_hadooptest --cluster theoden -Dtest=TestSleepJobCLI
\$ run_hadooptest --cluster theoden --mvn --nopasswd -w `pwd` -t TestEndToEndPipes
\$ run_hadooptest --cluster theoden --mvn -Dtest=TestSleepJobCLI
\$ run_hadooptest --cluster theoden --mvn -f /homes/hadoopqa/hadooptest.conf -Dtest=TestSleepJobCLI
\$ run_hadooptest -c theoden -j -n -Dtest=hadooptest.regression.TestVersion
\$ run_hadooptest -c theoden -m -P jacoco -Dtest=TestVersion
\$ run_hadooptest -c theoden -m -P clover -Djava.awt.headless=true -Dtest=TestVersion

EOF
    die($err_msg) if ($err_msg);
    exit 0;
}

sub writeSparkConf {
    my ($outfile, $version, $hadoopversion) = @_;
    if (-e $outfile) {
        execute("rm $outfile");
    }
    my $examples= "spark-examples.jar";
    open (FILE, ">> $outfile") || die "problem opening $outfile\n";
    print FILE "USER=hadoopqa\n";
    print FILE "LOG_LEVEL=DEBUG\n";
    print FILE "CLUSTER_TYPE=hadooptest.cluster.hadoop.fullydistributed.FullyDistributedCluster\n";
    print FILE "CHECK_CLUSTER_STATE=false\n";
    if ($version) {
        print FILE "SPARK_BIN=/home/y/share/spark/bin/spark-class\n";
        print FILE "SPARK_SUBMIT_BIN=/home/y/share/spark/bin/spark-submit\n";
        print FILE "SPARK_HOME=/home/y/share/spark/\n";
        print FILE "SPARK_EXAMPLES_JAR=/home/y/share/spark/lib/$examples\n";
    }
    close(FILE);
}

sub execute {
    my ($command) = @_;
    note($command);
    system($command);
}

my $cluster;
my $conf = glob("~/hadooptest.conf");
my $use_mvn = 1;
my $gdm = 0;
my $gdmVersion = 0;
my $storm = 0;
my $storm090 = 0;
my $stormconf;
my $tez = 0;
# $hadoop26 $hadoop27 $hadoop28 are now deprecated
my $hadoop26 = 0;
my $hadoop27 = 0;
my $hadoop28 = 0;
my $pig = 0;
my $spark = 0;
my $spark12 = 0;
my $spark13 = 0;
my $spark14 = 0;
my $spark15 = 0;
my $sparktag;
my $sparkhadoopversion;
my $sparkversionfull;
my $sparkversion;
my $sparkbranch;
my $workspace = "$Bin/..";
my $adm_box = "devadm102.blue.ygrid.yahoo.com";
my $nk = 0;
my $username = getpwuid($<);
my $nopasswd = ($username eq "hadoopqa") ? 1 : 0;
my $test;
my $gridName;
my $timeStamp;
my $sparkconf = "conf/hadooptest/hadooptest_spark_regression_config_hadoop_2/hadooptest_spark_ver.conf";

#
# Command line options processing
#
use Getopt::Long;
&Getopt::Long::Configure( 'pass_through');
my $result =
GetOptions(\%options,
    "cluster|c=s"        => \$cluster,
    "conf|f=s"           => \$conf,
    "mvn|m"              => sub { $use_mvn = 1 },
    "java|j"             => sub { $use_mvn = 0 },
    "gdm"                => \$gdm,
    "gdmVersion"          => \$gdmVersion,
    "tez"                => \$tez,
    "hadoop26"           => \$hadoop26,
    "hadoop27"           => \$hadoop27,
    "hadoop28"           => \$hadoop28,
    "pig"                => \$pig,
    "spark"              => \$spark,
    "spark12"            => \$spark12,
    "spark13"            => \$spark13,
    "spark14"            => \$spark14,
    "spark15"            => \$spark15,
    "sparktag=s"       => \$sparktag,
    "sparkfullver=s"   => \$sparkversionfull,
    "sparkhadoopver=s" => \$sparkhadoopversion,
    "sparkver=s"       => \$sparkversion,
    "sparkbranch=s"    => \$sparkbranch,
    "storm"              => \$storm,
    "storm090"           => \$storm090,
    "stormconf|y=s"      => \$stormconf,
    "nopasswd|n"         => \$nopasswd,
    "workspace|w=s"      => \$workspace,
    "adm_box|a=s"        => \$adm_box,
    "nk"                 => \$nk,
    "test|t=s"           => \$test,
    "help|h|?"
    ) or usage(1);
usage() if $options{help};
usage("Invalid arguments!!!") if (!$result);
usage("ERROR: Required cluster value not defined!!!") if (!defined($cluster));

if ($sparkversion) {
  # require other parameters to be specified
  #
  # example usage:
  #  scripts/run_hadooptest -c omegas -w /grid/0/tmp/hadooptest-hadoopqa-omegas/hadooptest 
  # -sparkver 1.5.1.1 -sparkhadoopver 2.6.0.16.1506060127 
  # -sparkfullver 1.5.1.1_2.6.0.16.1506060127_1510071630 
  # -sparktag yspark_yarn_1_5_latest_hadoop2 -sparkbranch quarantine 
  # -m -hadoop26 -t TestSparkHdfsLrCli
  note("you must specify all sparkver, sparkfullver and sparkhadoopver");
  if (!$sparkversionfull || !$sparkhadoopversion) {
       exit 1;
  }
  writeSparkConf("$workspace/$sparkconf", $sparkversion, $sparkhadoopversion);
  $conf = "$workspace/$sparkconf";
}

note("cluster='$cluster'");
note("conf = $conf");
note("workspace='$workspace'");
note("adm box = $adm_box");
note("use_mvn='$use_mvn'");

my @tests = ($test) ? split(",", $test) :
    (
     "hadooptest.regression.TestVersion",
     "hadooptest.regression.yarn.TestSleepJobAPI"
    );

my @classpath= (
    "/home/y/lib/jars/junit.jar",
    "/home/gs/gridre/yroot.$cluster/conf/hadoop/",
    "/tmp/RandomWriter.jar",
    "/home/gs/tez/current/",
    "/home/gs/tez/current/lib" );

unless ($use_mvn) {
    push(@classpath, "$workspace/target/hadooptest-1.0-SNAPSHOT.jar");
    push(@classpath, "$workspace/target/hadooptest-1.0-SNAPSHOT-tests.jar");
}

my $hadoop_install="/home/gs/gridre/yroot.$cluster";
my @hadoop_jar_paths = (
    "$hadoop_install/share/hadoop/share/hadoop/common/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/common",
    "$hadoop_install/share/hadoop-*/share/hadoop/common/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/hdfs",
    "$hadoop_install/share/hadoop-*/share/hadoop/hdfs/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/yarn",
    "$hadoop_install/share/hadoop-*/share/hadoop/yarn/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/mapreduce",
    "$hadoop_install/share/hadoop-*/share/hadoop/mapreduce/lib",
    "$hadoop_install/share/hadoop-*/share/hadoop/hdfs"
    );

foreach my $path (@hadoop_jar_paths) {
    push(@classpath, `echo -en $path`.'/*');
};

note("CLASSPATH=".join(":",@classpath));

if($storm||$storm090) {
	my $host = hostname;
	my $host_prefix = substr($host, 0, index($host, '.'));
	if (not $nk) {
	    execute("stty -echo; kinit -kt /homes/hadoopqa/hadoopqa.dev.headless.keytab hadoopqa\@DEV.YGRID.YAHOO.COM; stty echo");}
}
else {
	execute("stty -echo; kinit $username\@DS.CORP.YAHOO.COM; stty echo")
    	unless ($nopasswd);
}

$cmd = "ls -l /home/gs/gridre/yroot.$cluster/share/hadoop|cut -d'>' -f2|cut -d'-' -f2";
note($cmd);
chomp($hadoop_version = `$cmd`);
note("Hadoop version = '$hadoop_version'");

if (($hadoop26) || ($hadoop27) || ($hadoop28)) {
    note("WARN: Deprecated parameters are used {-hadoop26, -hadoop27, -hadoop28}!!!");
    note("WARN: This will now be auto-set based on the installed hadoop version '$hadoop_version' on the cluster '$cluster'.");
}
if ($hadoop_version =~ '^2.6') {
    $hadoop26=1;
}
elsif ($hadoop_version =~ '^2.7') {
    $hadoop27=1;
} elsif ($hadoop_version =~ '^2.8') {
    $hadoop28=1;
}

if ($hadoop27) {
    # Copy a hadoop 2.7+ specific capacity-scheduler.xml over the existing one to support a change
    # in yarn.scheduler.capacity.maximum-am-resource-percent from 0.1 to 0.3 in hadoop 2.7 
    note("copy updated capacity-scheduler.xml for hadoop 2.7 over existing single-queue capacity-scheduler.xml:");
    copy("$workspace/conf/SingleQueueConf/single-queue-capacity-scheduler.xml.hadoop27plus","$workspace/conf/SingleQueueConf/single-queue-capacity-scheduler.xml");
}

if ($hadoop28) {
    # Copy a hadoop 2.8+ specific capacity-scheduler.xml over the existing one to support a change
    # in yarn.scheduler.capacity.maximum-am-resource-percent from 0.1 to 0.3 in hadoop 2.8 
    note("copy updated capacity-scheduler.xml for hadoop 2.8 over existing single-queue capacity-scheduler.xml:");
    copy("$workspace/conf/SingleQueueConf/single-queue-capacity-scheduler.xml.hadoop28plus","$workspace/conf/SingleQueueConf/single-queue-capacity-scheduler.xml");
}

# If the user has provided the -spark option to the script, install the Spark
# corresponding spark package so that we can run tests against the Spark jars.
if ($sparkversion) {
    execute("WORKSPACE=$workspace rm -rf $workspace/htf-common/src/test/scala/hadooptest/spark/regression/spark1_2");
    if ($sparkversionfull) {
        if ($sparkbranch) {
            execute("yinst i yspark_yarn-$sparkversionfull -live -downgrade -br $sparkbranch");
        } else {
            execute("yinst i yspark_yarn-$sparkversionfull -live -downgrade");
        }
    }
} elsif ($sparktag) {
    execute("WORKSPACE=$workspace rm -rf $workspace/htf-common/src/test/scala/hadooptest/spark/regression/spark1_2");
    if ($sparkbranch) {
      execute("yinst i yspark_yarn -tag $sparktag -live -downgrade -br $sparkbranch");
    } else {
      execute("yinst i yspark_yarn -tag $sparktag -live -downgrade -br quarantine");
    }
    # now that we have installed spark generate the conf again to make sure version in conf matches what was
    # installed with tag
    $sparkversionfull = `yinst ls yspark_yarn | awk '{print $1}' | cut -d '-' -f 2`;
    $sparkversion = `yinst ls yspark_yarn | awk '{print $1}' | cut -d '-' -f 2 | cut -d '_' -f 1`;
    $sparkhadoopversion = `yinst ls yspark_yarn | awk '{print $1}' | cut -d '-' -f 2 | cut -d '_' -f 2`;
    chomp($sparkversionfull);
    chomp($sparkversion);
    chomp($sparkhadoopversion);
    writeSparkConf("$workspace/$sparkconf", $sparkversion, $sparkhadoopversion);
    $conf = "$workspace/$sparkconf";
} elsif ($spark14) {
    execute("WORKSPACE=$workspace rm -rf $workspace/htf-common/src/test/scala/hadooptest/spark/regression/spark1_2");
    execute("yinst i yspark_yarn -tag yspark_yarn_1_4_latest_hadoop2 -live -downgrade -br quarantine");
} elsif ($spark15) {
    execute("WORKSPACE=$workspace rm -rf $workspace/htf-common/src/test/scala/hadooptest/spark/regression/spark1_2");
    execute("yinst i yspark_yarn -tag yspark_yarn_1_5_latest_hadoop2 -live -downgrade -br quarantine");
} else {
    writeSparkConf("$workspace/$sparkconf");
    $conf = "$workspace/$sparkconf";
    execute("WORKSPACE=$workspace rm -rf $workspace/htf-common/src/test/scala/hadooptest/spark/regression/spark1_2");
}

# Temporarily removing help:active-profiles as the maven-help-plugin is not
# resolving from an offline local repository.  It keeps trying to download
# the metadata from a versionless instance of the plugin.
# my $pom_opt="-f pom.xml help:active-profiles";
my $pom_opt="-f pom.xml";
my $mvn_settings_opt="-gs $workspace/resources/yjava_maven/settings.xml.gw ";
my $java_lib_path="-Djava.library.path=/home/gs/gridre/yroot.$cluster/share/hadoop/lib/native/Linux-amd64-64/";

# Create the local repo directory on /grid/0/ and set the maven.repo.local
# option.  This is necessary so we don't store the maven local repo on the
# /homes NFS mount by default, which causes performance degradation,
# especially on OpenStack.
execute("mkdir /grid/0/tmp/htf_mvn_repo");
my $dependencies_opt = "-Dmaven.repo.local=/grid/0/tmp/htf_mvn_repo";

# If we are building spark, we require the correct spark dependencies to be picked up.
if ($spark) {
    $dependencies_opt .= " -Pprofile-spark";
}

# This relies on the hadoop options adding the correct dependencies below
if ($sparkversion) {
    $dependencies_opt .= " -Dspark.hadoop.version=$sparkhadoopversion -Dspark.version=$sparkversion -Dspark.version.full=$sparkversionfull";
}
elsif ($spark14) {
    $dependencies_opt .= " -Dspark.hadoop.version=2.6.0.16.1506060127 -Dspark.version=1.4.0.0 -Dspark.version.full=1.4.0.0_2.6.0.16.1506060127_1506241920";
}
elsif ($spark15) {
    $dependencies_opt .= " -Dspark.hadoop.version=2.6.0.16.1506060127 -Dspark.version=1.5.1.0 -Dspark.version.full=1.5.1.0_2.6.0.16.1506060127_1509251420";
}

if ($storm) {
	# When Storm is used, this profile must be called before profile-ci
	# or any other profile.  If another profile is used first, the
	# dependency jars from that profile might have Storm configuration
	# or classes that will upset the classpath assembled by maven,
	# and can lead to unexpected configuration or classes ending up in
	# the classpath before the necessary Storm support.  By putting
	# profile-storm first in the maven profile order, it ensures
	# that Storm API calls and configuration will be using the
	# deployed Storm jars. -->
    $dependencies_opt .= " -Pprofile-storm -Pprofile-all";
}
elsif ($storm090) {
	# When Storm is used, this profile must be called before profile-ci
	# or any other profile.  If another profile is used first, the
	# dependency jars from that profile might have Storm configuration
	# or classes that will upset the classpath assembled by maven,
	# and can lead to unexpected configuration or classes ending up in
	# the classpath before the necessary Storm support.  By putting
	# profile-storm first in the maven profile order, it ensures
	# that Storm API calls and configuration will be using the
	# deployed Storm jars. -->
    $dependencies_opt .= " -Pprofile-storm-090 -Pprofile-all";
}
elsif ($tez){
    $dependencies_opt .= " -Pprofile-tez -Pprofile-hadoop-2-6-plus -Pprofile-ci -Pprofile-all";
}
elsif ($spark) {
    $dependencies_opt .= " -Pprofile-spark  -Pprofile-hadoop-2-7 -Pprofile-ci -Pprofile-all";
}
elsif ($hadoop28) {
    $dependencies_opt .= " -Pprofile-hadoop-2-8 -Pprofile-ci -Pprofile-all";
}
elsif ($spark) {
    $dependencies_opt .= " -Pprofile-spark  -Pprofile-hadoop-2-7 -Pprofile-ci -Pprofile-all";
}
elsif ($hadoop27) {
    $dependencies_opt .= " -Pprofile-hadoop-2-7 -Pprofile-ci -Pprofile-all";
}
elsif ($hadoop26) {
    $dependencies_opt .= " -Pprofile-hadoop-2-6-plus -Pprofile-ci -Pprofile-all";
}
else {
	# When Hadoop is used, this profile must be called before profile-ci
	# or any other profile.  If another profile is used first, the
	# dependency jars from that profile might have Hadoop configuration
	# or classes that will upset the classpath assembled by maven,
	# and can lead to unexpected configuration or classes ending up in
	# the classpath before the necessary Hadoop support.  By putting
	# profile-hadoop first in the maven profile order, it ensures
	# that Hadoop API calls and configuration will be using the
	# deployed Hadoop jars. -->
    $dependencies_opt .= " -Pprofile-hadoop -Pprofile-ci -Pprofile-all";
}

my $stormClusterConf =
    (($stormconf) && (-e "$stormconf")) ? "-DSTORM_CLUSTER_CONF=$stormconf" : '';

my @common_args = (
    "-DCLUSTER_NAME=$cluster",
    "-DWORKSPACE=$workspace",
    "-DADM_BOX=$adm_box",
    "-Dhadooptest.config=$conf",
    $stormClusterConf,
    $dependencies_opt,
    join(" ", @ARGV)
    );

my $test_opt = ($use_mvn) ?
    "-Dtest=".join(",",@tests) :
    join(" ", @tests);

# TODO: investigate merging this in build.xml.
if ( grep( /^jacoco$/, @ARGV ) ) {
    # raw results directory
    my $jacoco_result_dir="/homes/hadoopqa/jacoco/results/latest";
    execute("rm -f $jacoco_result_dir/*");

    # final results directory
    my $jacoco_result_file="$workspace/target/jacoco.exec";
    execute("rm -f $jacoco_result_file");

    # misc results directory:
    # $jacoco_result_dir="$workspace/data/exec";
    # execute("rm -f $jacoco_result_dir/*");
    # $jacoco_result_dir="$workspace/data/exec/jacoco.exec";
    # execute("rm -f $jacoco_result_dir");
}

# If the user has provided the -pig option to the script, install the Pig
# dependencies, for ABF feed, that are needed for the Pig scripts,
# invoked from within HTF. This does not deploy actual Pig package, because
# the Jenkins jobs are expected to care for that. This just cares for an internal
# HTF test dependency.
if ($pig) {
    execute("yinst i fetl_base_feed_jar");
    execute("yinst i fetl_projector");
    execute("yinst i htf_pig_QE_data -br current");
}

if ($tez){
  $cmd = "ls -l /home/gs/tez/current/tez-api-*.jar | sed -e's/  */ /g' | cut -d' ' -f9| cut -d'-' -f3| rev | cut -d. -f2- | rev";
  note($cmd);
  chomp($tez_version = `$cmd`);
  note("Tez version = '$tez_version'");
}

if ($use_mvn) {
    # RUN TESTS VIA MAVEN
    execute("/usr/local/bin/yinst install yjava_maven -br test -yes") unless (-e "/home/y/bin/mvn");

    unless ($storm||$storm090) {
        $pom_opt .= " -Pprofile-hadoop";
    }

    my $hadoop_share = "/home/gs/gridre/yroot.$cluster/share/hadoop-$hadoop_version/share/hadoop/";
    if ($gdm) {
        # use -gdmVersion to run gdm testcases against GDM 6.3 or before that. 
        if ($gdmVersion) {
            print "==================================================================== \n ";
            my $base_dir = "/grid/0/yroot/var/yroots/console/home/y/libexec/prod_hadoop_versions/";
            my $find_rule = File::Find::Rule->new;
            $find_rule->maxdepth(1);

            # Only return directories
            $find_rule->directory;

            # Apply the rule and retrieve the subdirectories, which should be the
            # directories named by Hadoop version.
            my @sub_dirs = $find_rule->in($base_dir);

            my @basedirs;
            foreach my $dirname (@sub_dirs) {
                push @basedirs, basename($dirname);
            }

            # inverse alphabetical sort the names so most recent version comes first
            foreach my $basedir (@basedirs) {
                if ($basedir =~ m/^2\.(.*)/) {
                    print "\n\nSetting GDM Hadoop dependencies to Hadoop version $basedir\n\n";
                    $hadoop_version = $basedir;
                    note("Hadoop version = '$hadoop_version'");
                    $hadoop_share = "/grid/0/yroot/var/yroots/console/home/y/libexec/prod_hadoop_versions/$hadoop_version/share/hadoop-$hadoop_version/share/hadoop/";
                    last;
                }
            }
        } else {
            # Get yroot name , hadoop version and create the hadoop share location.
            $hadoop_share = "/home/gs/gridre/yroot.$cluster/share/hadoop/share/hadoop/";
        }
    }
    print "hadoop_share = $hadoop_share \n";

    my @settings = (
        "CLUSTER=$cluster",
        "HADOOP_VERSION=$hadoop_version",
        "HADOOP_INSTALL=$hadoop_install",
        "HADOOP_SHARE=$hadoop_share",
        "HTF_WORKSPACE=$workspace" );
    push(@settings, "TEZ_VERSION=$tez_version") if ($tez_version);
    my $settings_str = join(" ", @settings);

    # Call the script that gets a number of required system dependencies
    # through "yinst fetch".
    if ($sparkversion) {
      execute("WORKSPACE=$workspace $workspace/scripts/ci_install_jar sparkversion $sparkversionfull $sparkbranch");
    } elsif ($spark14) {
      execute("WORKSPACE=$workspace $workspace/scripts/ci_install_jar spark14");
    } elsif ($spark15) {
      execute("WORKSPACE=$workspace $workspace/scripts/ci_install_jar spark15");
    } else {
      execute("WORKSPACE=$workspace $workspace/scripts/ci_install_jar");
    }

    if ($storm||$storm090) {
        # Clean the project and build the jars so that we can have a test
        # jar with dependencies for Storm topologies.
        execute("HTF_WORKSPACE=$workspace /home/y/bin/mvn $pom_opt $mvn_settings_opt clean package -DskipTests $dependencies_opt -Dmaven.compiler.source=1.8 -Dmaven.compiler.target=1.8");
    }
    else {
        # Just clean the project.
        execute("${settings_str} ".
                "/home/y/bin/mvn $pom_opt $mvn_settings_opt clean package ".
                "-DskipTests $dependencies_opt ".
                "-Dmaven.compiler.source=1.8 ".
                "-Dmaven.compiler.target=1.8");
    }
    print "clean up.. \n";
    system("sh" , "./scripts/htf_cleanup.sh");
    print "-------------------------------------------------------------------------------------------------- \n";
 
    ############################################################################
    # Execute the tests.
    ############################################################################
    execute("${settings_str} ".
            "HADOOP_CONF_DIR=/home/gs/gridre/yroot.${cluster}/conf/hadoop " .
            "HADOOP_HOME=/home/gs/gridre/yroot.${cluster}/share/hadoop " .
            "HADOOP_PREFIX=/home/gs/gridre/yroot.${cluster}/share/hadoop " .
            "/home/y/bin/mvn $pom_opt $mvn_settings_opt ".
            "-Dmaven.compiler.source=1.8 ".
            "-Dmaven.compiler.target=1.8 ".
            "test -DfailIfNoTests=false ".
            join(" ", @common_args) . " " .
            $test_opt);

} else {

    # Need to compile the hadooptest jar if it doesn't exist.
    unless (-e "$workspace/target/hadooptest-1.0-SNAPSHOT.jar") {
        execute("/home/y/bin/mvn clean package -DskipTests -Dmaven.compiler.source=1.8 -Dmaven.compiler.target=1.8");
    }

    # RUN TESTS VIA JAVA
    execute("/home/y/bin/java -cp ".
            join(":",@classpath)." ".
            join(" ",@common_args)." ".
            "$java_lib_path ".
            "org.junit.runner.JUnitCore ".
            $test_opt);
}

# Write test results in TAP format to /homes/hadoopqa

# Copy the finger print file if it exists (so this test execution can be tied
# with other related Jenkins jobs.
# E.g "/home/y/var/builds/workspace/NightlyHadoopQEAutomation-23
my $result_root_dir = "/homes/hadoopqa/hudson_artifacts/";
$result_root_dir .= ($hadoop_version =~ /^0.23/) ? "hudson_artifacts-0.23" : "hudson_artifacts-2.0";
my $fingerprint = $result_root_dir."/artifacts.stamp";
if (-e $fingerprint) {
    note("copy fingerprint file '$fingerprint' to workspace '$workspace':");
    copy($fingerprint,"$workspace/artifacts.stamp");
} else {
    note("Found no upstream fingerprint file '$fingerprint' to copy to workspace '$workspace':");
}
