#!/bin/bash

# need the cluster name we're running against
if [ $# -ne 1 ]; then
  echo "Error: need the CLUSTER name we are running on"
  exit 1
fi
CLUSTER=$1
echo "Our test cluster to run against is: $CLUSTER"


# script to run each of the testcases
# this assumes you are on a node of the cluster you're running against, and that
# the 'buildme' script has successfully run in this path
#
# normal usage is something like `runme $> /tmp/logfile`

# store case by case results
TMPFILE="/tmp/doas_token_test.tmp"
echo "Temp logfile is: $TMPFILE"


#
# Setup the test's unix and kerberos IDs and credentials
#
# run orig user as dfsload
# Note, this is done to ensure a headless user and access to test data, another unix
# id and kerb principal can be used, but they must have matching ssh keys, kerb princ,
# and the test data likewise needs to have perms match
if [ "`whoami`" != "dfsload" ]; then
  echo "Error: need to run as unix id 'dfsload'"
  exit 1
fi
# get tgt
kinit -k -t /homes/dfsload/dfsload.dev.headless.keytab dfsload@DEV.YGRID.YAHOO.COM


# get hadoop version
/home/gs/gridre/yroot.$CLUSTER/share/hadoop/bin/hadoop --config /home/gs/gridre/yroot.$CLUSTER/conf/hadoop/ version

# gen our input test data for the java wordcounts
TMPDATAFILE="/tmp/tmpdatafile1.txt"
echo "" > $TMPDATAFILE
for LINE in {1..10};do
  echo "$LINE: This is a test line of text that we want to count quite a bit but lots for now then what what" >> $TMPDATAFILE
done
# rmdir --ignore-fail-on-non-empty doesn't seem to work...
# Todo, could use 'fs -rm -r -f' here 
/home/gs/gridre/yroot.$CLUSTER/share/hadoop/bin/hadoop --config /home/gs/gridre/yroot.$CLUSTER/conf/hadoop/ fs -rm  -skipTrash /data/in/*
/home/gs/gridre/yroot.$CLUSTER/share/hadoop/bin/hadoop --config /home/gs/gridre/yroot.$CLUSTER/conf/hadoop/ fs -rmdir /data/in
/home/gs/gridre/yroot.$CLUSTER/share/hadoop/bin/hadoop --config /home/gs/gridre/yroot.$CLUSTER/conf/hadoop/ fs -mkdir /data/in
/home/gs/gridre/yroot.$CLUSTER/share/hadoop/bin/hadoop --config /home/gs/gridre/yroot.$CLUSTER/conf/hadoop/ fs -chmod 777 /data/in
echo "Creating input test data files..."
if [ $? -ne 0 ];then
  echo "Error: mkdir for test input data failed"
  exit 1
fi
for FILE in {1..5}; do
  /home/gs/gridre/yroot.$CLUSTER/share/hadoop/bin/hadoop --config /home/gs/gridre/yroot.$CLUSTER/conf/hadoop/ fs -put $TMPDATAFILE /data/in/file$FILE
done
/home/gs/gridre/yroot.$CLUSTER/share/hadoop/bin/hadoop --config /home/gs/gridre/yroot.$CLUSTER/conf/hadoop/ fs -chmod 777 /data/in/*
# done gen'ing data


# write header for run report 
echo "++++ Doas, Proxy-Doas and Token Test Run, using HDFS and WebHDFS ++++" > /tmp/doas_token_test.tmp
# run each of the testcases
for TESTCASE in `ls *.jar|cut -d '.' -f1`;do
  echo "Running testcase: $TESTCASE"
  /home/gs/gridre/yroot.$CLUSTER/share/hadoop/bin/hadoop --config /home/gs/gridre/yroot.$CLUSTER/conf/hadoop/ jar  $TESTCASE.jar $TESTCASE
  RESULT=$?
  if [ "$RESULT" -ne 0 ]; then
    echo "Testcase $TESTCASE: Failed, error code is: $RESULT"
    echo "Testcase $TESTCASE: Failed, error code is: $RESULT" >> $TMPFILE
  else
     echo "Testcase $TESTCASE: Passed"
     echo "Testcase $TESTCASE: Passed" >> $TMPFILE
  fi 
  echo ""
done

# display result
echo "Run completed, results from $TMPFILE:"
cat $TMPFILE
